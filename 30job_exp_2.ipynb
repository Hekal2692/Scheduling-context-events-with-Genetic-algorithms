{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0xXpsWF28y04",
        "I0BkG22LWHTv",
        "9-0bfMugPmPS",
        "wiZdZU9G_Iei",
        "ubjEqTwsGVnI",
        "REwLw7n5lXp3",
        "UZjOUyUjnAtJ",
        "8xgXVLs2dG3T",
        "WqNno4-Kdawz",
        "4oo6MyTediHN",
        "7DRNkW60eWAj",
        "lqZuOikGfblN",
        "gqsAs4-mf_U9",
        "SVDYf3T6gvn4",
        "DLLR03MJhHzK"
      ],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Includes factoring out the communication cost as the accumulative sum of the applying the time delay to the ESs that require receiving context events"
      ],
      "metadata": {
        "id": "7AnKsa8V1T5I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Libraries and loading the JSON file"
      ],
      "metadata": {
        "id": "0xXpsWF28y04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Approach creating task graph\n",
        "!pip install numpy\n",
        "!pip install pygad\n",
        "!pip install deap\n",
        "import numpy as np\n",
        "import pygad\n",
        "import random\n",
        "import json\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from decimal import Decimal\n",
        "from copy import deepcopy\n",
        "from collections import defaultdict\n",
        "\n",
        "from functools import partial\n",
        "from deap import base, creator, tools, algorithms\n",
        "\n",
        "#Read example JSON file.\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Open the file from Google Drive\n",
        "with open('/content/drive/MyDrive/example_50_N1.json', 'r') as f:\n",
        "#with open('/content/drive/MyDrive/example1.json', 'r') as f:\n",
        "# Load the JSON data from the file\n",
        "    json_data = json.load(f)"
      ],
      "metadata": {
        "id": "tD8vfkZB8tcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "465e00f6-d293-4bf5-8604-bd59b573eeda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Collecting pygad\n",
            "  Downloading pygad-3.2.0-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.8/80.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from pygad) (2.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from pygad) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pygad) (1.23.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pygad) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pygad) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pygad) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pygad) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pygad) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pygad) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pygad) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pygad) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->pygad) (1.16.0)\n",
            "Installing collected packages: pygad\n",
            "Successfully installed pygad-3.2.0\n",
            "Collecting deap\n",
            "  Downloading deap-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deap) (1.23.5)\n",
            "Installing collected packages: deap\n",
            "Successfully installed deap-1.4.1\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Scheduling the Exchange of Context Informationfor Time-Triggered Adaptive Systems**"
      ],
      "metadata": {
        "id": "CUqgbWeHbJ63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions"
      ],
      "metadata": {
        "id": "OAOiInIY_HkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Read_Parent_AM(json_data):     # Returning a dictionary about the Application graph\n",
        "  AMx = json_data['application']\n",
        "  #print(\"The Parent AM is \",AMx)\n",
        "  return AMx\n",
        "\n",
        "def Read_Parent_PM(json_data):     # Returning a dictionary about the Platform graph\n",
        "  PMx = json_data['platform']\n",
        "  return PMx\n",
        "\n",
        "\n",
        "def construct_communication_costs_from_json(json_data):\n",
        "  messages = json_data['application']['messages']\n",
        "  communication_costs = {}\n",
        "\n",
        "  for message in messages:\n",
        "      sender = message['sender']\n",
        "      receiver = message['receiver']\n",
        "      size = message['size']\n",
        "\n",
        "      if sender not in communication_costs:\n",
        "          communication_costs[sender] = {receiver: size}\n",
        "      else:\n",
        "          communication_costs[sender][receiver] = size\n",
        "\n",
        "  return communication_costs\n",
        "\n",
        "def construct_task_dag_from_json(APP_MODEL): # where APP_MODELis an instance from the function Read_Parent_AM(json_data)\n",
        "    # this function returns 2 lists one for task_dag (list of lists) for the successors\n",
        "    # Another list tis the wcet_values showing the worst excution times for each job\n",
        "    jobs = APP_MODEL['jobs']\n",
        "    messages = APP_MODEL['messages']\n",
        "\n",
        "    num_tasks = len(jobs)\n",
        "\n",
        "    # Create a mapping of sender and receiver tasks for each message\n",
        "    message_mapping = {}\n",
        "    for message in messages:\n",
        "        sender = message['sender']\n",
        "        receiver = message['receiver']\n",
        "        if sender not in message_mapping:\n",
        "            message_mapping[sender] = [receiver]\n",
        "        else:\n",
        "            message_mapping[sender].append(receiver)\n",
        "\n",
        "    # Create the task DAG\n",
        "    task_dag = [[] for _ in range(num_tasks)]\n",
        "\n",
        "    for job_id, successors in message_mapping.items():\n",
        "        task_dag[job_id] = successors\n",
        "\n",
        "    # Extract the WCET values\n",
        "    wcet_values = [job['processing_times'] for job in jobs]\n",
        "\n",
        "    return task_dag, wcet_values\n",
        "\n",
        "\n",
        "def extract_message_list(APP_MODEL):\n",
        "  # Returns a list of dictionaries for each meassage attribute where the keys for each message will be\n",
        "  # id,sender,reciever,size\n",
        "    messages = APP_MODEL['messages']                              # feteching messages (list of dictionaries)\n",
        "    task_ids = [job['id'] for job in APP_MODEL['jobs']]           # creating a list of task_ids\n",
        "    message_list = []                                                            # Initializing a list\n",
        "    for msg in messages:\n",
        "        sender_id = task_ids.index(msg['sender'])\n",
        "        receiver_id = task_ids.index(msg['receiver'])\n",
        "        message_size = msg['size']\n",
        "        message_id = msg['id']\n",
        "        message_info = {\n",
        "            'id': message_id,\n",
        "            'sender': sender_id,\n",
        "            'receiver': receiver_id,\n",
        "            'size': message_size\n",
        "        }\n",
        "        message_list.append(message_info)\n",
        "    return message_list\n",
        "\n",
        "def compute_makespan(schedule):    # passing the Re-construction function result\n",
        "    # Extract end times from the schedule\n",
        "    end_times = [info[2] for info in schedule.values()]\n",
        "    # The makespan is the maximum end time\n",
        "    makespan = max(end_times)\n",
        "    return makespan\n",
        "\n",
        "def plot_schedule(schedule):\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    processors = sorted(list(set([processor for processor, _, _ in schedule.values()])))\n",
        "    colors = plt.cm.tab10(np.linspace(0, 1, len(processors)))\n",
        "\n",
        "    for task, (processor, start_time, end_time) in schedule.items():\n",
        "        color = colors[processor]\n",
        "        ax.plot([start_time, end_time], [task, task], label=f'Processor {processor}', linewidth=10, marker='o', color=color)\n",
        "\n",
        "    ax.set_xlabel('Time')\n",
        "    ax.set_ylabel('Task')\n",
        "\n",
        "    # Calculate the makespan and set x-axis limit\n",
        "    makespan = max(end_time for _, (_, _, end_time) in schedule.items())\n",
        "    ax.set_xlim(0, makespan)\n",
        "\n",
        "    # Set y-axis ticks and labels\n",
        "    plt.yticks(range(len(schedule)))\n",
        "    plt.grid()\n",
        "    plt.title(\"Task Schedule\")\n",
        "\n",
        "    # Create a custom legend without duplicate labels\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    by_label = dict(zip(labels, handles))\n",
        "    ax.legend(by_label.values(), by_label.keys())\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def construct_graph_from_json(PLAT_MODEL):         # Function for constructing the PM , returns a graph object\n",
        "    # Extract nodes and links from JSON data\n",
        "    nodes = PLAT_MODEL['nodes']\n",
        "    links = PLAT_MODEL['links']\n",
        "\n",
        "    # Create an empty graph\n",
        "    graph = nx.Graph()\n",
        "\n",
        "    # Add nodes to the graph\n",
        "    for node in nodes:\n",
        "        node_id = node['id']\n",
        "        node_type = 'processor' if not node['is_router'] else 'switch'\n",
        "        graph.add_node(node_id, node_type=node_type)\n",
        "\n",
        "    # Add edges (links) to the graph\n",
        "    for link in links:\n",
        "        start = link['start']\n",
        "        end = link['end']\n",
        "        graph.add_edge(start, end)\n",
        "\n",
        "    return graph\n",
        "\n",
        "def generate_all_path_indexes_with_costs(graph):                           # Passing the PM as an argument , geneartes all paths between processors in the graph while calcualting the cost\n",
        "    processors = [node for node, data in graph.nodes(data=True) if data['node_type'] == 'processor']\n",
        "    # creating a list with the processor nodes ids\n",
        "    switches = [node for node, data in graph.nodes(data=True) if data['node_type'] == 'switch']\n",
        "    # creating a list with the switche nodes ids\n",
        "\n",
        "    def find_all_paths(source, target, path=[]): # A recursive function to find all paths between a give source node and target node, source and target are the iterables from the below for loop\n",
        "      # This function uses a depth first search (DFS) to explore the paths\n",
        "        path = path + [source]                    # Appending the \"source\" node to the path list, to keep track of nodes vistied in the list\n",
        "        if source == target:\n",
        "            return [path]\n",
        "        if source not in graph:                    # checks for node presence in graph\n",
        "            return []\n",
        "        paths = []\n",
        "        for node in graph[source]:                 # iterating through the current neighbors in graph\n",
        "            if node not in path:\n",
        "                newpaths = find_all_paths(node, target, path)  # calling the fuction recursively taking the neighbor node as a source node , target node is the same as the old one, path= path calculated so far\n",
        "                for newpath in newpaths:\n",
        "                    paths.append(newpath)\n",
        "        return paths\n",
        "\n",
        "    path_indexes = {}                             # Initialzing a dict. to store results (paths and costs)\n",
        "    path_id = 0\n",
        "    for source in processors:                     # iterating through the processors list in the PM [0,1,2]\n",
        "        for target in processors:                 # iterating through processors list in PM [0,1,2], done to consider all pairs between source and target nodes\n",
        "            if source != target:                  # checking if source and target nodes are not the same as we don't need a path from the processor to itself\n",
        "                all_paths = find_all_paths(source, target)\n",
        "                all_paths = [path for path in all_paths if any(node in path for node in switches)] # filtering the paths to keep the ones with only one switch node\n",
        "                if all_paths:\n",
        "                    for path in all_paths:\n",
        "                        # Compute the cost as the number of edges in the path\n",
        "                        path_cost = len(path) - 1                                   # Computing the cost by subtracting 1 from the number of nodes in the path.\n",
        "                        # Add the path, its ID, and its cost to the result\n",
        "                        path_indexes[path_id] = {\"path\": path, \"cost\": path_cost}\n",
        "                        path_id += 1\n",
        "    return path_indexes\n",
        "\n",
        "#Python function for n-bit\n",
        "def value_to_nbit_vector(value, n): # representing teh context event in bit form\n",
        "    binary = bin(value)[2:]  # Remove the '0b' prefix\n",
        "    binary = binary.zfill(n)  # Pad with leading zeros to make it n bits long\n",
        "    bit_vector = [int(bit) for bit in binary]  # Convert each character to integer, bit representation\n",
        "    return bit_vector\n",
        "\n",
        "#python function to return the highest bit set (To decode which context event we are dealing with. The value of c)\n",
        "def find_msb_position(bit_vector):\n",
        "    msb_position = 0\n",
        "    for i in range(len(bit_vector)):\n",
        "        if bit_vector[i] == 1:\n",
        "            msb_position = len(bit_vector) - i\n",
        "            break\n",
        "    return msb_position\n",
        "\n",
        "#function to convert bit_vector to integer\n",
        "def bit_vector_to_value(bit_vector):\n",
        "    value = 0\n",
        "    n = len(bit_vector)\n",
        "    for i in range(n):\n",
        "        value += bit_vector[i] * (2 ** (n - i - 1))\n",
        "    return value\n",
        "\n",
        "\n",
        "#This function is to find the equivalent binary value of c bit pattern\n",
        "def msb_only_bit_vector(bit_vector):\n",
        "    # Find the position of the most significant bit\n",
        "    msb_position = find_msb_position(bit_vector)\n",
        "\n",
        "    # Create a new bit vector with all bits set to 0\n",
        "    new_bit_vector = [0] * len(bit_vector)\n",
        "\n",
        "    # Set only the most significant bit to 1\n",
        "    if msb_position > 0:\n",
        "        new_bit_vector[-msb_position] = 1\n",
        "\n",
        "    return new_bit_vector\n",
        "\n",
        "def getparent(p):# where B is the Database indices and p is the schedule index\n",
        "  # Assuming p is integer\n",
        "  Schedule_index = p\n",
        "  #print('Schedule_index', Schedule_index)\n",
        "  Context_event_dec = find_msb_position(Schedule_index)\n",
        "  #print('Context_event_dec', Context_event_dec)\n",
        "\n",
        "\n",
        "  Context_event_Bin = msb_only_bit_vector(Schedule_index)\n",
        "  #print(\"Context_event_Bin\",Context_event_Bin)\n",
        "  Parent_Schedule_Bin = sub_list(Schedule_index,Context_event_Bin)\n",
        "  #print(\"Parent_Schedule_Bin\",Parent_Schedule_Bin)\n",
        "  Parent_Schedule_dec = bit_vector_to_value(Parent_Schedule_Bin)\n",
        "  #print(\"Parent_Schedule_dec\",Parent_Schedule_dec)\n",
        "  return Parent_Schedule_Bin,Parent_Schedule_dec\n",
        "\n",
        "def sub_list(list1, list2):                                                # Helper function called in getpredecessors(p)\n",
        "    if len(list1) != len(list2):\n",
        "        raise ValueError(\"Input lists must have the same length\")\n",
        "\n",
        "    result = [a - b for a, b in zip(list1, list2)]\n",
        "    return result\n",
        "\n",
        "def Apply_Context(AMx,PMx,p,contextList): # AMx= application model , PMx = Platform model, p = Parent Schedule index in binary\n",
        "  jobs = AMx['jobs']\n",
        "  #print(\"The p in the Apply_Context function is \",p)\n",
        "  context_event = find_msb_position(p)\n",
        "  #print(\"The context event  is :\",context_event)\n",
        "  for context in range(len(contextList)):\n",
        "    if context == context_event and contextList[context][0] == 0:\n",
        "      #print(\"The event {} is a SLACK!!\".format(contextList[context]))\n",
        "      for job in jobs:\n",
        "        #print(job)\n",
        "        if job[\"id\"] == contextList[context][1]:\n",
        "          job[\"processing_times\"]= int(round(job[\"processing_times\"]* (100-contextList[context][2])/100))\n",
        "\n",
        "\n",
        "          #print(\"The new AM is : \",AMx)\n",
        "    elif context == context_event and contextList[context][0] == 1:\n",
        "      print(\"The event {} is a Failure!!\".format(contextList[context]))\n",
        "  #print(\"Function Apply_Context successively applied!!\")\n",
        "  return AMx,PMx\n",
        "\n",
        "def getPredecessors(p,B): # where the first argument is Binary Encoding of the Schedule, and the second argumet is the Dataset\n",
        "  Parent_Schedule_Bin,Parent_Schedule_dec = getparent(p) # Calling the helper function to return the parent schedule as decimal and binary value\n",
        "  #print(\"The Parent Schedule of {} is {} in binary format and is {} in decimal format\".format(p,Parent_Schedule_Bin,Parent_Schedule_dec))   # print check\n",
        "  context_c = find_msb_position(Parent_Schedule_Bin)    # Getting the context event value of the parent schedule\n",
        "  #print(\"The context event of the Parent Schedule is : \",context_c)                      # print check\n",
        "  for i in range (len(B)): # iterating through the dataset\n",
        "    if B[i][3] == Parent_Schedule_dec:\n",
        "      AM,PM = B[i][1], B[i][2]\n",
        "      #print(\"Function getPredecessor successfuly deployed \")\n",
        "      #print(\"The AMx is {} and the PMx is {}\".format(AMx,PMx))\n",
        "\n",
        "      return AM,PM\n",
        "      #print(\"Function getPredecessor successfuly deployed \")\n",
        "  return None,None\n",
        "\n",
        "\n",
        "def GA_SOLVE(AMy,PMy,SMx,Data,contextList,Current_Context_event):                             # 29.09.23 ==> adding the Context List &  Current_Context_event attributes\n",
        "  successors, processing_times = construct_task_dag_from_json(AMy)\n",
        "  message_list = extract_message_list(AMy)\n",
        "  PLATFORM_GRAPH_OBJECT = construct_graph_from_json(PMy)\n",
        "  all_path_indexes_with_costs = generate_all_path_indexes_with_costs(PLATFORM_GRAPH_OBJECT)\n",
        "  n_tasks = len(processing_times)\n",
        "\n",
        "  #SMy = generate_schedule_base_schedule(processing_times, message_list, all_path_indexes_with_costs)  # INVOKING GA\n",
        "  SMy, Genome, affected_list2 = generate_Successor_schedule(processing_times, message_list, all_path_indexes_with_costs,Data,contextList,Current_Context_event)       # 29.09.23 Adding the context lIst & Current_Context_event attributes\n",
        "  return SMy , Genome ,affected_list2\n",
        "\n",
        "# TEST FUNCTIONS\n",
        "\n",
        "#AMx = Read_Parent_AM(json_data)\n",
        "#PMx = Read_Parent_PM(json_data)\n",
        "##print(AMx)\n",
        "#print(PMx)\n",
        "#type(AMx)\n",
        "#successors, processing_times = construct_task_dag_from_json(AMx)\n",
        "#print(successors)\n",
        "#print(processing_times)\n",
        "#message_list = extract_message_list(AMx)\n",
        "#print(message_list)\n",
        "#PLATFORM_GRAPH_OBJECT = construct_graph_from_json(PMx)\n",
        "#print(PLATFORM_GRAPH_OBJECT)\n",
        "#all_path_indexes_with_costs = generate_all_path_indexes_with_costs(PLATFORM_GRAPH_OBJECT)\n",
        "#print(all_path_indexes_with_costs)"
      ],
      "metadata": {
        "id": "uy5JX9as3eq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions Instances"
      ],
      "metadata": {
        "id": "I0BkG22LWHTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AMx = Read_Parent_AM(json_data)\n",
        "PMx = Read_Parent_PM(json_data)\n",
        "successors, processing_times = construct_task_dag_from_json(AMx)\n",
        "message_list = extract_message_list(AMx)\n",
        "PLATFORM_GRAPH_OBJECT = construct_graph_from_json(PMx)\n",
        "all_path_indexes_with_costs = generate_all_path_indexes_with_costs(PLATFORM_GRAPH_OBJECT)\n",
        "n_tasks = len(processing_times)"
      ],
      "metadata": {
        "id": "qiVwQ7DUWGZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algorithm (3)\n",
        "Schedule Reconstruction"
      ],
      "metadata": {
        "id": "9-0bfMugPmPS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwiSQ6whPfm_"
      },
      "outputs": [],
      "source": [
        "def reconstruct_schedule_with_precedenceX(task_allocation, node_list, processing_times, message_list, message_path_index, all_path_indexes_with_costs, message_priority_ordering):\n",
        "    # Create a deep copy of the message_list to avoid modifying the original data\n",
        "    message_list_copy = deepcopy(message_list)            #creating a copy of the message list\n",
        "\n",
        "    num_processors = max(task_allocation) + 1             # specifing the number of processors\n",
        "    processors = [[] for _ in range(num_processors)]      # creating a list of empty lists of size = processors\n",
        "\n",
        "    for task, processor in enumerate(task_allocation):    # Appending the list of processor\n",
        "        processors[processor].append(task)\n",
        "\n",
        "    schedule = {}                                         # Initializing a dictionary for the new schedule.\n",
        "    task_completion_times = [0] * len(node_list)          # Initializing a list of zeros with size = node list\n",
        "    message_dict = defaultdict(list)                      # creating an instance of defaultdict, it allows define a default value for keys that do not yet exist in the dictionary.\n",
        "    for i in range(n_tasks):\n",
        "        message_dict[i] = []                              # Initializing with all potential receivers\n",
        "\n",
        "\n",
        "    # Create a dictionary to map message ID to priority\n",
        "    #message_priority_dict = {idx: priority for priority, idx in enumerate(message_priority_ordering)}\n",
        "    # Modify the message_priority_dict to use actual message IDs\n",
        "    message_priority_dict = {message_id: priority for priority, message_id in enumerate(message_priority_ordering)}       # Mapping messages to priorities\n",
        "\n",
        "\n",
        "    for idx, message in enumerate(message_list_copy):\n",
        "        # Decode the message path index to the corresponding path index\n",
        "        path_id = message_path_index[idx]\n",
        "        path = all_path_indexes_with_costs[path_id][\"path\"]\n",
        "\n",
        "        path_cost= all_path_indexes_with_costs[path_id][\"cost\"]\n",
        "\n",
        "        # Adjust the size of the message with the cost of the path\n",
        "        message[\"size\"] += path_cost\n",
        "\n",
        "        # Append a tuple containing the sender, message size, and message priority to the receiver's list in the message_dict\n",
        "        #message_dict[message[\"receiver\"]].append((message[\"sender\"], message[\"size\"], message_priority_dict[idx]))\n",
        "\n",
        "        # Use message[\"id\"] to get the priority from message_priority_dict\n",
        "        message_dict[message[\"receiver\"]].append((message[\"sender\"], message[\"size\"], message_priority_dict[message[\"id\"]]))\n",
        "\n",
        "    # Sort each receiver's list in message_dict by message priority\n",
        "    for receiver, messages in message_dict.items():\n",
        "        message_dict[receiver] = sorted(messages, key=lambda x: x[2])\n",
        "\n",
        "    current_time_per_processor = [0] * num_processors                 # initilaizing a list of zeros of size = No. of processors\n",
        "    completed_tasks = set()                                           # creating an empty set for the completed tasks\n",
        "    ready_tasks = set(range(len(node_list)))                          # A set of random numbers of size = length of node list\n",
        "\n",
        "    while ready_tasks:                                                # iterating through the set of tasks\n",
        "        task = ready_tasks.pop()                                      # Acessing a task randomly and removing it from ready tasks set\n",
        "        task_id = node_list[task]                                     # getting the task id\n",
        "        processor = task_allocation[task]                             # Acessing the processor on which task should run\n",
        "        i = processor                                                 # Assigning the value of processor to i\n",
        "\n",
        "        predecessors = message_dict[task_id]                          # returns a tuple of [(sender,size,id)]\n",
        "        if all(p in completed_tasks for p, _, _ in predecessors):     # Checks if all pre-decessors(sender in the tuple of predecessors) are in completed tasks set\n",
        "            if predecessors:\n",
        "                # Ensure that the start_time is at least the end_time of the last task on this processor\n",
        "                start_time = max(current_time_per_processor[i], max(task_completion_times[sender] + size for sender, size, _ in predecessors))\n",
        "            else:\n",
        "                start_time = current_time_per_processor[i]\n",
        "\n",
        "            end_time = start_time + processing_times[task_id]\n",
        "            schedule[task_id] = (i, start_time, end_time)\n",
        "            task_completion_times[task_id] = end_time\n",
        "\n",
        "            current_time_per_processor[i] = end_time\n",
        "            completed_tasks.add(task_id)\n",
        "        else:\n",
        "            ready_tasks.add(task)\n",
        "\n",
        "    return schedule"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Base Schedule (Algorithm (2))"
      ],
      "metadata": {
        "id": "wiZdZU9G_Iei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_schedule_base_schedule(processing_times, message_list, all_path_indexes_with_costs):\n",
        "\n",
        "  creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))     # creating a FitnessMin class for minimization and setting the weights to (-1.0,).\n",
        "  creator.create(\"Individual\", list, fitness=creator.FitnessMin)  # creating the Individual class as a list with the FitnessMin class as the fitness attribute.\n",
        "\n",
        "  toolbox = base.Toolbox()\n",
        "\n",
        "  #parameters\n",
        "  num_tasks = len(processing_times)                               # No of jobs in task graph\n",
        "  nodes = json_data[\"platform\"][\"nodes\"]                          # Accessing the nodes of the Platform model\n",
        "  #processors = [node for node in nodes if not node[\"is_router\"]]  # Creating  a list of the processors in the Platform model.\n",
        "  num_machines = 2                              # A variable specifing the number of machines\n",
        "  num_message = len(message_list)                                 # A varible = No. of messages\n",
        "  num_paths = len(all_path_indexes_with_costs)                    # total number of paths\n",
        "\n",
        "  # Define the initialization function for the individual\n",
        "  def init_individual():\n",
        "      return random.sample(range(num_tasks), num_tasks)           # returns a list of random numbers in the range of num_tasks\n",
        "\n",
        "  print('The initial Indidvidual is {}'.format(init_individual()))  # OMAR\n",
        "\n",
        "  # Register the initialization function in the DEAP toolbox\n",
        "  toolbox.register(\"task_order\", init_individual)\n",
        "\n",
        "  #toolbox.register(\"task_order\", tools.initIterate, creator.Individual, init_individual)\n",
        "\n",
        "  #toolbox.register(\"task_order\", random.sample, range(num_tasks), num_tasks)\n",
        "\n",
        "  # Processor_allocation value encoding\n",
        "  def processor_allocation(n_task, n_machines):\n",
        "      return [random.randint(0, num_machines) for _ in range(n_task)]   # A random list , contains the allocation of each job with the corresponding task\n",
        "  toolbox.register(\"processor_allocation\", processor_allocation, n_task=num_tasks, n_machines=num_machines)\n",
        "  HH = toolbox.processor_allocation()                                                     # OMAR\n",
        "  #print('The randomly initialized task allocation looks as the follows {}'.format(HH))    # OMAR\n",
        "\n",
        "  # Message_path_index value encoding\n",
        "  def message_path_index(n_messages, max_all_path_index):\n",
        "      return [random.randint(0, max_all_path_index - 1) for _ in range(n_messages)]  # Ensure indices are within range\n",
        "      # returns a list of random numbers of size = n_messages and values ranging from (0, max_all_path_index - 1).\n",
        "\n",
        "  toolbox.register(\"message_path_index\", message_path_index, n_messages=num_message, max_all_path_index= num_paths)\n",
        "\n",
        "  #print('The randomly initialized message path looks as the follows {}'.format(toolbox.message_path_index()))    # OMAR\n",
        "\n",
        "\n",
        "  # Message_priority_ordering permutation encoding\n",
        "  def message_priority_ordering(n_messages):\n",
        "      return random.sample(range(n_messages), n_messages)         # returns a list of size = n_messages, showing the messages ordering\n",
        "  toolbox.register(\"message_priority_ordering\", message_priority_ordering, n_messages=num_message)\n",
        "\n",
        "  #print('The random priority ordering looks as follows {}'.format(toolbox.message_priority_ordering()))        # OMAR\n",
        "\n",
        "  # Combined Individual\n",
        "  def create_individual():\n",
        "      individual = []\n",
        "      individual.extend(toolbox.task_order())\n",
        "      individual.extend(toolbox.processor_allocation())\n",
        "      individual.extend(toolbox.message_path_index())\n",
        "      individual.extend(toolbox.message_priority_ordering())\n",
        "      return individual\n",
        "  # To view the whole indivudual\n",
        "  #individual = create_individual()\n",
        "  #print(\"Individual:\", individual)\n",
        "  toolbox.register(\"individual\", tools.initIterate, creator.Individual, create_individual)\n",
        "\n",
        "  #print('The initial combined individual is {}'.format(toolbox.individual()))                  # OMAR\n",
        "\n",
        "  # Population initialization\n",
        "  toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "  #print('The initial population is {}'.format(toolbox.population(n=100)))\n",
        "\n",
        "  def reconstruct_schedule(processor_allocation, task_order):\n",
        "      # Initialize the schedule as a dictionary of empty lists\n",
        "      # where each key is a processor id\n",
        "      schedule = {i: [] for i in range(max(processor_allocation) + 1)}\n",
        "\n",
        "      # Iterate over tasks in their given order\n",
        "      for task in task_order:\n",
        "          # Assign each task to its processor\n",
        "          schedule[processor_allocation[task]].append(task)\n",
        "\n",
        "\n",
        "\n",
        "      return schedule\n",
        "  #print ('The Initial Schedule looks as {}'.format(reconstruct_schedule(toolbox.processor_allocation(), toolbox.task_order())))     # OMAR\n",
        "\n",
        "  ############################################################################################################################################################\n",
        "\n",
        "  ####################################################### Evaluation, selection, mating, and mutation methods ################################################\n",
        "\n",
        "  #########compute makespan function ################################################\n",
        "  def compute_makespan(schedule):\n",
        "      # Extract end times from the schedule\n",
        "      end_times = [info[2] for info in schedule.values()]\n",
        "      # The makespan is the maximum end time\n",
        "      makespan = max(end_times)\n",
        "      return makespan\n",
        "  ################################################################\n",
        "\n",
        "  # Define the evaluation function\n",
        "  def evaluate(individual, processing_times, message_list, all_path_indexes_with_costs):\n",
        "      # Split individual into its components\n",
        "      task_order_len = num_tasks                  # An Integer with the number of the tasks\n",
        "      processor_allocation_len = num_tasks        # Integer\n",
        "      message_path_index_len = num_message        # Integer\n",
        "      message_priority_ordering_len = num_message # Integer\n",
        "      task_order = individual[:task_order_len]    # permuation encoding, Acessing the first portion of the chromosome\n",
        "      processor_allocation = individual[task_order_len:task_order_len + processor_allocation_len] # value encoding acessing the second portion of chromosomes\n",
        "      message_path_index = individual[task_order_len + processor_allocation_len : task_order_len + processor_allocation_len + message_path_index_len] # value encoding\n",
        "      message_priority_ordering = individual[task_order_len + processor_allocation_len + message_path_index_len :] # permuation encoding\n",
        "      # print(\"task_order ================\", task_order)\n",
        "      # print(\"processor_allocation ================\", processor_allocation)\n",
        "      # print(\"message_path_index ================\", message_path_index)\n",
        "      # print(\"message_priority_ordering ================\", message_priority_ordering)\n",
        "      schedule = reconstruct_schedule_with_precedenceX(processor_allocation, task_order, processing_times, message_list, message_path_index, all_path_indexes_with_costs, message_priority_ordering)\n",
        "      makespan  = compute_makespan(schedule)\n",
        "      fitness = 1.0 / (makespan)\n",
        "      #print(\"Makespan over time is %s, %s\", makespan)\n",
        "      #scheduleR = reconstruct_schedule(processor_allocation, task_order)\n",
        "      # You would need to split the individual into its parts and use these parts to calculate the makespan\n",
        "      # For the sake of this example, let's just return the sum of all elements in the individual\n",
        "      return fitness,\n",
        "\n",
        "  # Register the evaluation function\n",
        "  toolbox.register(\"evaluate\", evaluate,processing_times=processing_times,\n",
        "                  message_list=message_list, all_path_indexes_with_costs=all_path_indexes_with_costs)\n",
        "\n",
        "  # Register selection operator\n",
        "  toolbox.register(\"select\", tools.selTournament, tournsize=3) # Applying tournament selection, tournsize=3\n",
        "                                                               # meaning three individuals will be randomly selected from the population,\n",
        "                                                               # and the best one among them will be chosen as a parent for the next generation.\n",
        "\n",
        "\n",
        "  # Register the crossover operator (Permutation encoded)\n",
        "  def mate(ind1, ind2, task_order_len):\n",
        "      # Copy the individuals\n",
        "      child1, child2 = toolbox.clone(ind1), toolbox.clone(ind2) # creating copies from ind1 and ind2 to preserve original originals while crossover and mutation.\n",
        "\n",
        "      # Apply PMX crossover to task_order\n",
        "      tools.cxPartialyMatched(child1[:task_order_len], child2[:task_order_len]) # Applying partially matched crossover method\n",
        "\n",
        "      # Return the modified individuals\n",
        "      return child1, child2\n",
        "\n",
        "  toolbox.register(\"mate\", mate, task_order_len=num_tasks)                # Register the crossover function\n",
        "\n",
        "\n",
        "  #toolbox.register(\"mate\", tools.cxTwoPoint)\n",
        "\n",
        "\n",
        "  # mutation operators\n",
        "  # def mutation_task_order(individual, task_order_len):\n",
        "  #     task_order = individual[:task_order_len] # permutation encoding\n",
        "  #     random.shuffle(task_order) # Shuffle the indexes of task_order\n",
        "  #     individual[:task_order_len] = task_order\n",
        "  #     return individual,\n",
        "  def mutation_task_order(individual, task_order_len):\n",
        "      task_order = individual[:task_order_len]  # permutation encoding\n",
        "      indices = random.sample(range(task_order_len), 2)  # Select two random indices\n",
        "      task_order[indices[0]], task_order[indices[1]] = task_order[indices[1]], task_order[indices[0]]  # Swap the elements at the selected indices\n",
        "      individual[:task_order_len] = task_order\n",
        "      return individual,\n",
        "\n",
        "  def mutation_processor_allocation(individual,task_order_len, processor_allocation_len): # (task_order_len = processor_allocation_len)\n",
        "      processor_allocation = individual[task_order_len:task_order_len + processor_allocation_len] # value encoding\n",
        "      tools.mutUniformInt(processor_allocation, low=0, up=1, indpb=0.05) # Mutate processor_allocation by uniformly changing its elements with an indpb chance\n",
        "      individual[task_order_len:task_order_len + processor_allocation_len] = processor_allocation\n",
        "      return individual,\n",
        "\n",
        "  def mutation_message_path_index(individual, task_order_len, processor_allocation_len, message_path_index_len):\n",
        "      message_path_index = individual[task_order_len + processor_allocation_len : task_order_len + processor_allocation_len + message_path_index_len] # value encoding\n",
        "      tools.mutUniformInt(message_path_index, low=0, up=1, indpb=0.05) # Mutate message_path_index by uniformly changing its elements with an indpb chance\n",
        "      individual[task_order_len + processor_allocation_len : task_order_len + processor_allocation_len + message_path_index_len] = message_path_index\n",
        "      return individual,\n",
        "\n",
        "  def mutation_message_priority_ordering(individual, task_order_len, processor_allocation_len, message_path_index_len):\n",
        "      message_priority_ordering = individual[task_order_len + processor_allocation_len + message_path_index_len :] # permuation encoding\n",
        "      tools.mutShuffleIndexes(message_priority_ordering, indpb=0.05) # Mutate message_priority_ordering by shuffling its indexes with an indpb chance\n",
        "      individual[task_order_len + processor_allocation_len + message_path_index_len :] = message_priority_ordering\n",
        "      return individual,\n",
        "\n",
        "  # register the mutation operators\n",
        "  toolbox.register(\"mutate_task_order\", mutation_task_order, task_order_len = num_tasks)\n",
        "  toolbox.register(\"mutate_processor_allocation\", mutation_processor_allocation, task_order_len = num_tasks, processor_allocation_len = num_tasks)\n",
        "  toolbox.register(\"mutate_message_path_index\", mutation_message_path_index, task_order_len = num_tasks, processor_allocation_len = num_tasks, message_path_index_len = num_message)\n",
        "  toolbox.register(\"mutate_message_priority_ordering\", mutation_message_priority_ordering, task_order_len = num_tasks, processor_allocation_len = num_tasks, message_path_index_len = num_message)\n",
        "\n",
        "  ######################################################\n",
        "  # Create an initial population\n",
        "  pop = toolbox.population(n=100)\n",
        "  # Evaluate the entire population\n",
        "  fitnesses = map(toolbox.evaluate, pop)\n",
        "  for ind, fit in zip(pop, fitnesses):\n",
        "      ind.fitness.values = fit\n",
        "  # Crossover probability and mutation probability\n",
        "  CXPB, MUTPB = 0.3, 0.2\n",
        "\n",
        "  # Extract all the fitnesses of\n",
        "  fits = [ind.fitness.values[0] for ind in pop]\n",
        "\n",
        "  NGEN = 50 #Number of generations\n",
        "  # Begin the evolution\n",
        "  for g in range(NGEN):\n",
        "      # A new generation\n",
        "      offspring = toolbox.select(pop, len(pop)) # Apply the tournmnet selection , 3 individulas will be selected\n",
        "      # Clone the selected individuals\n",
        "      offspring = list(map(toolbox.clone, offspring))\n",
        "\n",
        "      # Apply crossover and mutation on the offspring\n",
        "      for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
        "          if random.random() < CXPB:\n",
        "              toolbox.mate(child1, child2) # Applying crossover for Permutation encoding (task orders)\n",
        "              del child1.fitness.values\n",
        "              del child2.fitness.values\n",
        "      # mutation\n",
        "      for mutant in offspring:\n",
        "          if random.random() < MUTPB:\n",
        "              toolbox.mutate_task_order(mutant)\n",
        "              toolbox.mutate_processor_allocation(mutant)\n",
        "              toolbox.mutate_message_path_index(mutant)\n",
        "              toolbox.mutate_message_priority_ordering(mutant)\n",
        "              del mutant.fitness.values\n",
        "\n",
        "      # Evaluate the individuals with an invalid fitness\n",
        "      invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
        "      fitnesses = map(toolbox.evaluate, invalid_ind)\n",
        "      for ind, fit in zip(invalid_ind, fitnesses):\n",
        "          ind.fitness.values = fit\n",
        "      # The population is entirely replaced by the offspring\n",
        "      pop[:] = offspring\n",
        "  # Print the best individual\n",
        "  best_ind = tools.selBest(pop, 1)[0]\n",
        "  #print(\"Best individual is %s, %s\" % (best_ind, best_ind.fitness.values))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  task_order_len = num_tasks\n",
        "  processor_allocation_len = num_tasks\n",
        "  message_path_index_len = num_message\n",
        "  message_priority_ordering_len = num_message\n",
        "  task_order = best_ind[:task_order_len]\n",
        "  processor_allocation = best_ind[task_order_len:task_order_len + processor_allocation_len] #value encoding\n",
        "  message_path_index = best_ind[task_order_len + processor_allocation_len : task_order_len + processor_allocation_len + message_path_index_len] #value encoding\n",
        "  message_priority_ordering = best_ind[task_order_len + processor_allocation_len + message_path_index_len :] #permuation encoding\n",
        "  # print(\"task_order ================\", task_order)\n",
        "  # print(\"processor_allocation ================\", processor_allocation)\n",
        "  # print(\"message_path_index ================\", message_path_index)\n",
        "  # print(\"message_priority_ordering ================\", message_priority_ordering)\n",
        "  scheduleFinal = reconstruct_schedule_with_precedenceX(processor_allocation, task_order, processing_times, message_list, message_path_index, all_path_indexes_with_costs, message_priority_ordering)\n",
        "  #print('The final schedule is {}'.format(scheduleFinal))\n",
        "  Final_genome= [task_order,processor_allocation,message_path_index,message_priority_ordering]\n",
        "  return scheduleFinal , Final_genome\n",
        "\n",
        "\n",
        "\n",
        "#scheduleFinal, Final_genome= generate_schedule_base_schedule(processing_times, message_list, all_path_indexes_with_costs)\n",
        "\n",
        "#makespanFinal  = compute_makespan(scheduleFinal)\n",
        "\n",
        "#plot_schedule(scheduleFinal)\n",
        "#print('The final schedule makespan is {}'.format(makespanFinal))\n",
        "#print(\"The Final Genome for this schdule is {}\".format(Final_genome))"
      ],
      "metadata": {
        "id": "kd7a2WecAOAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Re-construction function with Helper function No.1 , Helper function No.2 ,Helper function No.3 + getting The Affected Endsystems"
      ],
      "metadata": {
        "id": "ubjEqTwsGVnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Update_Context_List(Schedule, contextList):\n",
        "    for i in range(len(contextList)):\n",
        "        schedule_info = Schedule.get(i)  # Get the value of the specified key\n",
        "        if schedule_info:\n",
        "            start_time = schedule_info[1]\n",
        "            end_time = schedule_info[2]\n",
        "            context_time = int(round(start_time + (1 - (contextList[i][2] / 100)) * (end_time - start_time)))\n",
        "            contextList[i] = contextList[i] + (context_time,)\n",
        "    return contextList\n",
        "\n",
        "def Get_Scheduled_task(Data,contextList,Current_Context_event):\n",
        "  Scheduled_tasks = []\n",
        "  schedule= Data[0]\n",
        "  for context in range (len(contextList)):\n",
        "    if Current_Context_event == context:\n",
        "      #print(contextList[context])\n",
        "      current_context = contextList[context]\n",
        "      for task in schedule:\n",
        "        #print(task)\n",
        "        if schedule[task][2] <= current_context[-1]:\n",
        "          #print(schedule[task])\n",
        "          Scheduled_tasks.append(task)\n",
        "  return Scheduled_tasks\n",
        "\n",
        "def update_lists(Scheduled_tasks, node_list, task_allocation):\n",
        "    # Create a copy of the original lists to avoid modifying them directly\n",
        "    updated_node_list = node_list.copy()\n",
        "    updated_task_allocation = task_allocation.copy()\n",
        "\n",
        "    for task in Scheduled_tasks:\n",
        "        while task in updated_node_list:\n",
        "            # Exclude all occurrences of the task from the node_list\n",
        "            index = updated_node_list.index(task)\n",
        "            updated_node_list.pop(index)\n",
        "\n",
        "            # Exclude the corresponding processor\n",
        "            updated_task_allocation.pop(index)\n",
        "\n",
        "    return updated_node_list, updated_task_allocation\n",
        "\n",
        "def filter_scheduled_tasks(Data, Scheduled_tasks):\n",
        "    schedule = Data[0]\n",
        "    scheduled_dict = {}\n",
        "\n",
        "    for task_id in Scheduled_tasks:\n",
        "        if task_id in schedule:\n",
        "            scheduled_dict[task_id] = schedule[task_id]\n",
        "\n",
        "    return scheduled_dict\n",
        "\n",
        "def Affected_ES(Data, node_list, task_allocation,message_path_index,message_priority_ordering):\n",
        "  GLOBAL_CHANGES_LIST = []\n",
        "\n",
        "  if ((Data[0] != node_list) or (Data[1] != task_allocation)):\n",
        "    #print(\"Condition 1 met (prioirities and allocations), ES list needs update\")\n",
        "    for i in range (len(Data[0])):\n",
        "        task_item = task_allocation[i]\n",
        "        if task_item not in GLOBAL_CHANGES_LIST:\n",
        "          GLOBAL_CHANGES_LIST.append(task_item)\n",
        "\n",
        "\n",
        "  if ((Data[2] != message_path_index) or (Data[3] != message_priority_ordering)):\n",
        "    #print(\"Condition 2 (path index and priority odering)  met, ES list needs update\")\n",
        "    for i in range (len(Data[2])):\n",
        "        sender = all_path_indexes_with_costs[message_path_index[i]][\"path\"][0]\n",
        "        reciever= all_path_indexes_with_costs[message_path_index[i]][\"path\"][-1]\n",
        "        if (sender not in GLOBAL_CHANGES_LIST) or (reciever not in GLOBAL_CHANGES_LIST):\n",
        "          GLOBAL_CHANGES_LIST.append(sender)\n",
        "          GLOBAL_CHANGES_LIST.append(reciever)\n",
        "  return GLOBAL_CHANGES_LIST\n",
        "\n",
        "def reconstruct_schedule_with_precedenceXX(task_allocation, node_list, processing_times, message_list, message_path_index, all_path_indexes_with_costs, message_priority_ordering,Data,contextList,Current_Context_event): # New 21-09-23\n",
        "  # Data ==> has only information about the previous schedule\n",
        "  # Create a deep copy of the message_list to avoid modifying the original data\n",
        "    message_list_copy = deepcopy(message_list)            #creating a copy of the message list\n",
        "\n",
        "    #print(\"The previous Data is : \",Data)\n",
        "\n",
        "\n",
        "\n",
        "   # Call Affected_ES function to get the list of affected processors\n",
        "    affected_processors = Affected_ES(Data, node_list, task_allocation, message_path_index, message_priority_ordering)\n",
        "\n",
        "\n",
        "    # New Modifications (1) 29-09-23\n",
        "    Scheduled_tasks = Get_Scheduled_task(Data,contextList,Current_Context_event)   # Returns a list of tasks that where previously scheduled\n",
        "    #print(\"The Scheduled Tasks are\",Scheduled_tasks )\n",
        "\n",
        "    # New Modification (2) 29-09-23\n",
        "    new_node_list , new_task_allocation = update_lists(Scheduled_tasks, node_list, task_allocation)\n",
        "    #print(\"The node_list after update is: \",new_node_list)\n",
        "    #print(\"The Task_allocation after update is:\", new_task_allocation)\n",
        "\n",
        "    # New Modification (3) 29.09.23\n",
        "    Partial_Schedule = filter_scheduled_tasks(Data, Scheduled_tasks)\n",
        "    #print(\"The unchanged part from the parent schedule is :\", Partial_Schedule)\n",
        "\n",
        "    num_processors = max(task_allocation) + 1             # specifing the number of processors\n",
        "    processors = [[] for _ in range(num_processors)]      # creating a list of empty lists of size = processors\n",
        "\n",
        "    for task, processor in enumerate(new_task_allocation):    # Appending the list of processor\n",
        "        processors[processor].append(task)\n",
        "\n",
        "    n_tasks = len(new_node_list)\n",
        "    schedule = {}                                         # Initializing a dictionary for the new schedule.\n",
        "    task_completion_times = [0] * len(node_list)          # Initializing a list of zeros with size = node list\n",
        "    message_dict = defaultdict(list)                      # creating an instance of defaultdict, it allows define a default value for keys that do not yet exist in the dictionary.\n",
        "    for i in range(n_tasks):\n",
        "        message_dict[i] = []                              # Initializing with all potential receivers\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Create a dictionary to map message ID to priority\n",
        "    #message_priority_dict = {idx: priority for priority, idx in enumerate(message_priority_ordering)}\n",
        "    # Modify the message_priority_dict to use actual message IDs\n",
        "    message_priority_dict = {message_id: priority for priority, message_id in enumerate(message_priority_ordering)}       # Mapping messages to priorities\n",
        "\n",
        "\n",
        "    for idx, message in enumerate(message_list_copy):\n",
        "        # Decode the message path index to the corresponding path index\n",
        "        path_id = message_path_index[idx]\n",
        "        path = all_path_indexes_with_costs[path_id][\"path\"]\n",
        "\n",
        "        path_cost= all_path_indexes_with_costs[path_id][\"cost\"]\n",
        "\n",
        "        # Adjust the size of the message with the cost of the path\n",
        "        message[\"size\"] += path_cost\n",
        "\n",
        "        # Append a tuple containing the sender, message size, and message priority to the receiver's list in the message_dict\n",
        "        #message_dict[message[\"receiver\"]].append((message[\"sender\"], message[\"size\"], message_priority_dict[idx]))\n",
        "\n",
        "        # Use message[\"id\"] to get the priority from message_priority_dict\n",
        "        message_dict[message[\"receiver\"]].append((message[\"sender\"], message[\"size\"], message_priority_dict[message[\"id\"]]))\n",
        "\n",
        "    # Sort each receiver's list in message_dict by message priority\n",
        "    for receiver, messages in message_dict.items():\n",
        "        message_dict[receiver] = sorted(messages, key=lambda x: x[2])\n",
        "\n",
        "    current_time_per_processor = [contextList[Current_Context_event][-1]] * num_processors    # 29.09.23 Updating the start time based on the context event time\n",
        "    #print(\"The current time per processor list \",current_time_per_processor)\n",
        "    completed_tasks = set()                                           # creating an empty set for the completed tasks\n",
        "    completed_tasks = set(Partial_Schedule.keys())                    # 09.10.2023 edited\n",
        "\n",
        "    ready_tasks = set(range(len(new_node_list)))                          # A set of random numbers of size = length of node list\n",
        "    #print(\"Ready tasks are\",ready_tasks)\n",
        "\n",
        "\n",
        "    while ready_tasks:                                                # iterating through the set of tasks\n",
        "        task = ready_tasks.pop()                                      # Acessing a task randomly and removing it from ready tasks set\n",
        "        #print(\"The Task in read_tasks look is :\",task)\n",
        "        task_id = new_node_list[task]                                     # getting the task id\n",
        "        #print(\"The task_id in ready_tasks loop is\",task_id)\n",
        "        processor = new_task_allocation[task]                             # Acessing the processor on which task should run\n",
        "        #print(\"The corresponding processor is :\",processor)\n",
        "        i = processor                                                 # Assigning the value of processor to i\n",
        "\n",
        "        # Introduce fixed delay for affected processors\n",
        "        if processor in affected_processors:\n",
        "            current_time_per_processor[processor] += 5\n",
        "        #print(\"The processor times are :\",current_time_per_processor)\n",
        "\n",
        "        predecessors = message_dict[task_id]                          # returns a tuple of [(sender,size,id)]\n",
        "        #print(\"The predecessors are \",predecessors)\n",
        "        if all(p in completed_tasks for p, _, _ in predecessors):     # Checks if all pre-decessors(sender in the tuple of predecessors) are in completed tasks set\n",
        "            #print(\"ïnside the if \")\n",
        "            if predecessors:\n",
        "                # Ensure that the start_time is at least the end_time of the last task on this processor\n",
        "                start_time = max(current_time_per_processor[i], max(task_completion_times[sender] + size for sender, size, _ in predecessors))\n",
        "                #print(\"The start_time is :\",start_time)\n",
        "            else:\n",
        "                start_time = current_time_per_processor[i]\n",
        "                #print(\"The start time in else is :\",start_time)\n",
        "\n",
        "            end_time = start_time + processing_times[task_id]\n",
        "            #print(\"The endtime is : \",end_time)\n",
        "            schedule[task_id] = (i, start_time, end_time)\n",
        "            task_completion_times[task_id] = end_time\n",
        "\n",
        "            current_time_per_processor[i] = end_time\n",
        "            completed_tasks.add(task_id)\n",
        "        else:\n",
        "            ready_tasks.add(task)\n",
        "    #print(\"The Previous genome is : \",Data[-1])\n",
        "\n",
        "    merged_schedule = deepcopy(Partial_Schedule)  # creating  a copy of Partial_Schedule (predecessor)\n",
        "    merged_schedule.update(schedule)\n",
        "\n",
        "    return merged_schedule, affected_processors\n",
        "\n",
        "\n",
        "# Test\n",
        "#task_allocation= [1, 0, 1, 1, 0, 0, 2, 2, 2, 2, 0]\n",
        "#node_list =      [9, 3, 7, 1, 0, 5, 4, 10, 8, 2, 6]\n",
        "#message_path_index = [0, 1, 0, 1, 1, 9, 1, 4, 1, 6, 1, 3, 0, 9]\n",
        "#message_priority_ordering = [2, 9, 1, 5, 13, 6, 7, 12, 11, 3, 10, 4, 8, 0]\n",
        "#Data = ({0: (0, 0, 2), 1: (1, 11, 15), 5: (1, 22, 25), 9: (1, 32, 35), 3: (1, 35, 40), 4: (1, 40, 44), 8: (1, 51, 55), 7: (1, 55, 60), 2: (1, 60, 64), 6: (1, 70, 72), 10: (1, 78, 80)}, {'jobs': [{'id': 0, 'processing_times': 2, 'mcet': 10, 'deadline': 250, 'can_run_on': [0, 1, 2]}, {'id': 1, 'processing_times': 2, 'mcet': 10, 'deadline': 250, 'can_run_on': [0, 1, 2]}, {'id': 2, 'processing_times': 4, 'mcet': 10, 'deadline': 300, 'can_run_on': [0, 1, 2]}, {'id': 3, 'processing_times': 5, 'mcet': 10, 'deadline': 256, 'can_run_on': [0, 1, 2]}, {'id': 4, 'processing_times': 4, 'mcet': 10, 'deadline': 256, 'can_run_on': [0, 1, 2]}, {'id': 5, 'processing_times': 3, 'mcet': 10, 'deadline': 256, 'can_run_on': [0, 1, 2]}, {'id': 6, 'processing_times': 2, 'mcet': 10, 'deadline': 256, 'can_run_on': [0, 1, 2]}, {'id': 7, 'processing_times': 5, 'mcet': 10, 'deadline': 256, 'can_run_on': [0, 1, 2]}, {'id': 8, 'processing_times': 4, 'mcet': 10, 'deadline': 256, 'can_run_on': [0, 1, 2]}, {'id': 9, 'processing_times': 3, 'mcet': 10, 'deadline': 256, 'can_run_on': [0, 1, 2]}, {'id': 10, 'processing_times': 2, 'mcet': 10, 'deadline': 256, 'can_run_on': [0, 1, 2]}], 'messages': [{'id': 0, 'sender': 0, 'receiver': 1, 'size': 6, 'timetriggered': True}, {'id': 1, 'sender': 0, 'receiver': 2, 'size': 2, 'timetriggered': True}, {'id': 2, 'sender': 0, 'receiver': 3, 'size': 3, 'timetriggered': True}, {'id': 3, 'sender': 0, 'receiver': 4, 'size': 4, 'timetriggered': True}, {'id': 4, 'sender': 1, 'receiver': 5, 'size': 4, 'timetriggered': True}, {'id': 5, 'sender': 1, 'receiver': 9, 'size': 6, 'timetriggered': True}, {'id': 6, 'sender': 2, 'receiver': 6, 'size': 2, 'timetriggered': True}, {'id': 7, 'sender': 3, 'receiver': 7, 'size': 4, 'timetriggered': True}, {'id': 8, 'sender': 4, 'receiver': 8, 'size': 3, 'timetriggered': True}, {'id': 9, 'sender': 4, 'receiver': 7, 'size': 2, 'timetriggered': True}, {'id': 10, 'sender': 5, 'receiver': 9, 'size': 4, 'timetriggered': True}, {'id': 11, 'sender': 6, 'receiver': 10, 'size': 2, 'timetriggered': True}, {'id': 12, 'sender': 7, 'receiver': 10, 'size': 3, 'timetriggered': True}, {'id': 13, 'sender': 9, 'receiver': 10, 'size': 2, 'timetriggered': True}]}, {'nodes': [{'id': 0, 'is_router': False}, {'id': 1, 'is_router': False}, {'id': 2, 'is_router': False}, {'id': 3, 'is_router': True}, {'id': 4, 'is_router': True}, {'id': 5, 'is_router': True}, {'id': 6, 'is_router': True}, {'id': 7, 'is_router': True}, {'id': 8, 'is_router': True}, {'id': 9, 'is_router': True}], 'links': [{'start': 0, 'end': 7}, {'start': 1, 'end': 7}, {'start': 7, 'end': 8}, {'start': 2, 'end': 8}, {'start': 3, 'end': 8}, {'start': 4, 'end': 8}, {'start': 8, 'end': 9}, {'start': 7, 'end': 9}, {'start': 5, 'end': 9}, {'start': 6, 'end': 9}], 'frequencies': [500, 1000], 'schemes': [{'id': 0, 'wcdt': 0, 'wcct': 0, 'wccr': 1}, {'id': 1, 'wcdt': 10, 'wcct': 10, 'wccr': 0.5}]}, 0, [[9, 3, 4, 8, 10, 7, 2, 6, 0, 1, 5], [1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], [1, 8, 5, 7, 1, 0, 5, 0, 9, 9, 8, 9, 0, 1], [6, 7, 0, 11, 2, 4, 3, 12, 1, 13, 5, 9, 8, 10]])\n",
        "\n",
        "#contextList = [(0, 0, 0, 2), (0, 1, 40, 13), (0, 2, 50, 37), (0, 3, 30, 60), (0, 4, 60, 41), (0, 5, 30, 25)]\n",
        "#Current_Context_event = 2\n",
        "#x = reconstruct_schedule_with_precedenceXX(task_allocation, node_list, processing_times, message_list, message_path_index, all_path_indexes_with_costs, message_priority_ordering,Data,contextList,Current_Context_event)\n",
        "#print(x)"
      ],
      "metadata": {
        "id": "aijAdRhOFIJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## New_GA\n",
        " In this GA, We add a new argument , which is DATA , from which we will access the Task_order,Processor_Allocaion, message_priority_ordering , message_path_index."
      ],
      "metadata": {
        "id": "REwLw7n5lXp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_Successor_schedule(processing_times, message_list, all_path_indexes_with_costs,Data,contextList,Current_Context_event): # 29.09.23 adding contextList & Current_Context_event as attributes\n",
        "\n",
        "  creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))     # creating a FitnessMin class for minimization and setting the weights to (-1.0,).\n",
        "  creator.create(\"Individual\", list, fitness=creator.FitnessMin)  # creating the Individual class as a list with the FitnessMin class as the fitness attribute.\n",
        "\n",
        "  toolbox = base.Toolbox()\n",
        "\n",
        "  #parameters\n",
        "  num_tasks = len(processing_times)                               # No of jobs in task graph\n",
        "  nodes = json_data[\"platform\"][\"nodes\"]                          # Accessing the nodes of the Platform model\n",
        "  #processors = [node for node in nodes if not node[\"is_router\"]]  # Creating  a list of the processors in the Platform model.\n",
        "  num_machines = 2                                # A variable specifing the number of machines\n",
        "  num_message = len(message_list)                                 # A varible = No. of messages\n",
        "  num_paths = len(all_path_indexes_with_costs)                    # total number of paths\n",
        "\n",
        "  # Define the initialization function for the individual\n",
        "  def init_individual():\n",
        "      return random.sample(range(num_tasks), num_tasks)           # returns a list of random numbers in the range of num_tasks\n",
        "\n",
        "  #print('The initial Indidvidual is {}'.format(init_individual()))  # OMAR\n",
        "\n",
        "  # Register the initialization function in the DEAP toolbox\n",
        "  toolbox.register(\"task_order\", init_individual)\n",
        "\n",
        "  #toolbox.register(\"task_order\", tools.initIterate, creator.Individual, init_individual)\n",
        "\n",
        "  #toolbox.register(\"task_order\", random.sample, range(num_tasks), num_tasks)\n",
        "\n",
        "  # Processor_allocation value encoding\n",
        "  def processor_allocation(n_task, n_machines):\n",
        "      return [random.randint(0, num_machines) for _ in range(n_task)]   # A random list , contains the allocation of each job with the corresponding task\n",
        "  toolbox.register(\"processor_allocation\", processor_allocation, n_task=num_tasks, n_machines=num_machines)\n",
        "  HH = toolbox.processor_allocation()                                                     # OMAR\n",
        "  #print('The randomly initialized task allocation looks as the follows {}'.format(HH))    # OMAR\n",
        "\n",
        "  # Message_path_index value encoding\n",
        "  def message_path_index(n_messages, max_all_path_index):\n",
        "      return [random.randint(0, max_all_path_index - 1) for _ in range(n_messages)]  # Ensure indices are within range\n",
        "      # returns a list of random numbers of size = n_messages and values ranging from (0, max_all_path_index - 1).\n",
        "\n",
        "  toolbox.register(\"message_path_index\", message_path_index, n_messages=num_message, max_all_path_index= num_paths)\n",
        "\n",
        "  #print('The randomly initialized message path looks as the follows {}'.format(toolbox.message_path_index()))    # OMAR\n",
        "\n",
        "\n",
        "  # Message_priority_ordering permutation encoding\n",
        "  def message_priority_ordering(n_messages):\n",
        "      return random.sample(range(n_messages), n_messages)         # returns a list of size = n_messages, showing the messages ordering\n",
        "  toolbox.register(\"message_priority_ordering\", message_priority_ordering, n_messages=num_message)\n",
        "\n",
        "  #print('The random priority ordering looks as follows {}'.format(toolbox.message_priority_ordering()))        # OMAR\n",
        "\n",
        "  # Combined Individual\n",
        "  def create_individual():\n",
        "      individual = []\n",
        "      individual.extend(toolbox.task_order())\n",
        "      individual.extend(toolbox.processor_allocation())\n",
        "      individual.extend(toolbox.message_path_index())\n",
        "      individual.extend(toolbox.message_priority_ordering())\n",
        "      return individual\n",
        "  # To view the whole indivudual\n",
        "  #individual = create_individual()\n",
        "  #print(\"Individual:\", individual)\n",
        "  toolbox.register(\"individual\", tools.initIterate, creator.Individual, create_individual)\n",
        "\n",
        "  #print('The initial combined individual is {}'.format(toolbox.individual()))                  # OMAR\n",
        "\n",
        "  # Population initialization\n",
        "  toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "  #print('The initial population is {}'.format(toolbox.population(n=100)))\n",
        "\n",
        "  def reconstruct_schedule(processor_allocation, task_order):\n",
        "      # Initialize the schedule as a dictionary of empty lists\n",
        "      # where each key is a processor id\n",
        "      schedule = {i: [] for i in range(max(processor_allocation) + 1)}\n",
        "\n",
        "      # Iterate over tasks in their given order\n",
        "      for task in task_order:\n",
        "          # Assign each task to its processor\n",
        "          schedule[processor_allocation[task]].append(task)\n",
        "\n",
        "\n",
        "\n",
        "      return schedule\n",
        "  #print ('The Initial Schedule looks as {}'.format(reconstruct_schedule(toolbox.processor_allocation(), toolbox.task_order())))     # OMAR\n",
        "\n",
        "  ############################################################################################################################################################\n",
        "\n",
        "  ####################################################### Evaluation, selection, mating, and mutation methods ################################################\n",
        "\n",
        "  #########compute makespan function ################################################\n",
        "  def compute_makespan(schedule):\n",
        "      # Extract end times from the schedule\n",
        "      end_times = [info[2] for info in schedule.values()]\n",
        "      # The makespan is the maximum end time\n",
        "      makespan = max(end_times)\n",
        "      return makespan\n",
        "  ################################################################\n",
        "\n",
        "  # Define the evaluation function\n",
        "  def evaluate(individual, processing_times, message_list, all_path_indexes_with_costs):\n",
        "      # Split individual into its components\n",
        "      task_order_len = num_tasks                  # An Integer with the number of the tasks\n",
        "      processor_allocation_len = num_tasks        # Integer\n",
        "      message_path_index_len = num_message        # Integer\n",
        "      message_priority_ordering_len = num_message # Integer\n",
        "      task_order = individual[:task_order_len]    # permuation encoding, Acessing the first portion of the chromosome\n",
        "      processor_allocation = individual[task_order_len:task_order_len + processor_allocation_len] # value encoding acessing the second portion of chromosomes\n",
        "      message_path_index = individual[task_order_len + processor_allocation_len : task_order_len + processor_allocation_len + message_path_index_len] # value encoding\n",
        "      message_priority_ordering = individual[task_order_len + processor_allocation_len + message_path_index_len :] # permuation encoding\n",
        "      # print(\"task_order ================\", task_order)\n",
        "      # print(\"processor_allocation ================\", processor_allocation)\n",
        "      # print(\"message_path_index ================\", message_path_index)\n",
        "      # print(\"message_priority_ordering ================\", message_priority_ordering)\n",
        "      schedule = reconstruct_schedule_with_precedenceX(processor_allocation, task_order, processing_times, message_list, message_path_index, all_path_indexes_with_costs, message_priority_ordering)\n",
        "      makespan  = compute_makespan(schedule)\n",
        "      fitness = 1.0 / (makespan)\n",
        "      #print(\"Makespan over time is %s, %s\", makespan)\n",
        "      #scheduleR = reconstruct_schedule(processor_allocation, task_order)\n",
        "      # You would need to split the individual into its parts and use these parts to calculate the makespan\n",
        "      # For the sake of this example, let's just return the sum of all elements in the individual\n",
        "      return fitness,\n",
        "\n",
        "  # Register the evaluation function\n",
        "  toolbox.register(\"evaluate\", evaluate,processing_times=processing_times,\n",
        "                  message_list=message_list, all_path_indexes_with_costs=all_path_indexes_with_costs)\n",
        "\n",
        "  # Register selection operator\n",
        "  toolbox.register(\"select\", tools.selTournament, tournsize=3) # Applying tournament selection, tournsize=3\n",
        "                                                               # meaning three individuals will be randomly selected from the population,\n",
        "                                                               # and the best one among them will be chosen as a parent for the next generation.\n",
        "\n",
        "\n",
        "  # Register the crossover operator (Permutation encoded)\n",
        "  def mate(ind1, ind2, task_order_len):\n",
        "      # Copy the individuals\n",
        "      child1, child2 = toolbox.clone(ind1), toolbox.clone(ind2) # creating copies from ind1 and ind2 to preserve original originals while crossover and mutation.\n",
        "\n",
        "      # Apply PMX crossover to task_order\n",
        "      tools.cxPartialyMatched(child1[:task_order_len], child2[:task_order_len]) # Applying partially matched crossover method\n",
        "\n",
        "      # Return the modified individuals\n",
        "      return child1, child2\n",
        "\n",
        "  toolbox.register(\"mate\", mate, task_order_len=num_tasks)                # Register the crossover function\n",
        "\n",
        "\n",
        "  #toolbox.register(\"mate\", tools.cxTwoPoint)\n",
        "\n",
        "\n",
        "  # mutation operators\n",
        "  # def mutation_task_order(individual, task_order_len):\n",
        "  #     task_order = individual[:task_order_len] # permutation encoding\n",
        "  #     random.shuffle(task_order) # Shuffle the indexes of task_order\n",
        "  #     individual[:task_order_len] = task_order\n",
        "  #     return individual,\n",
        "  def mutation_task_order(individual, task_order_len):\n",
        "      task_order = individual[:task_order_len]  # permutation encoding\n",
        "      indices = random.sample(range(task_order_len), 2)  # Select two random indices\n",
        "      task_order[indices[0]], task_order[indices[1]] = task_order[indices[1]], task_order[indices[0]]  # Swap the elements at the selected indices\n",
        "      individual[:task_order_len] = task_order\n",
        "      return individual,\n",
        "\n",
        "  def mutation_processor_allocation(individual,task_order_len, processor_allocation_len): # (task_order_len = processor_allocation_len)\n",
        "      processor_allocation = individual[task_order_len:task_order_len + processor_allocation_len] # value encoding\n",
        "      tools.mutUniformInt(processor_allocation, low=0, up=1, indpb=0.05) # Mutate processor_allocation by uniformly changing its elements with an indpb chance\n",
        "      individual[task_order_len:task_order_len + processor_allocation_len] = processor_allocation\n",
        "      return individual,\n",
        "\n",
        "  def mutation_message_path_index(individual, task_order_len, processor_allocation_len, message_path_index_len):\n",
        "      message_path_index = individual[task_order_len + processor_allocation_len : task_order_len + processor_allocation_len + message_path_index_len] # value encoding\n",
        "      tools.mutUniformInt(message_path_index, low=0, up=1, indpb=0.05) # Mutate message_path_index by uniformly changing its elements with an indpb chance\n",
        "      individual[task_order_len + processor_allocation_len : task_order_len + processor_allocation_len + message_path_index_len] = message_path_index\n",
        "      return individual,\n",
        "\n",
        "  def mutation_message_priority_ordering(individual, task_order_len, processor_allocation_len, message_path_index_len):\n",
        "      message_priority_ordering = individual[task_order_len + processor_allocation_len + message_path_index_len :] # permuation encoding\n",
        "      tools.mutShuffleIndexes(message_priority_ordering, indpb=0.05) # Mutate message_priority_ordering by shuffling its indexes with an indpb chance\n",
        "      individual[task_order_len + processor_allocation_len + message_path_index_len :] = message_priority_ordering\n",
        "      return individual,\n",
        "\n",
        "  # register the mutation operators\n",
        "  toolbox.register(\"mutate_task_order\", mutation_task_order, task_order_len = num_tasks)\n",
        "  toolbox.register(\"mutate_processor_allocation\", mutation_processor_allocation, task_order_len = num_tasks, processor_allocation_len = num_tasks)\n",
        "  toolbox.register(\"mutate_message_path_index\", mutation_message_path_index, task_order_len = num_tasks, processor_allocation_len = num_tasks, message_path_index_len = num_message)\n",
        "  toolbox.register(\"mutate_message_priority_ordering\", mutation_message_priority_ordering, task_order_len = num_tasks, processor_allocation_len = num_tasks, message_path_index_len = num_message)\n",
        "\n",
        "  ######################################################\n",
        "  # Create an initial population\n",
        "  pop = toolbox.population(n=100)\n",
        "  # Evaluate the entire population\n",
        "  fitnesses = map(toolbox.evaluate, pop)\n",
        "  for ind, fit in zip(pop, fitnesses):\n",
        "      ind.fitness.values = fit\n",
        "  # Crossover probability and mutation probability\n",
        "  CXPB, MUTPB = 0.3, 0.2\n",
        "\n",
        "  # Extract all the fitnesses of\n",
        "  fits = [ind.fitness.values[0] for ind in pop]\n",
        "\n",
        "  NGEN = 50 #Number of generations\n",
        "  # Begin the evolution\n",
        "  for g in range(NGEN):\n",
        "      # A new generation\n",
        "      offspring = toolbox.select(pop, len(pop)) # Apply the tournmnet selection , 3 individulas will be selected\n",
        "      # Clone the selected individuals\n",
        "      offspring = list(map(toolbox.clone, offspring))\n",
        "\n",
        "      # Apply crossover and mutation on the offspring\n",
        "      for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
        "          if random.random() < CXPB:\n",
        "              toolbox.mate(child1, child2) # Applying crossover for Permutation encoding (task orders)\n",
        "              del child1.fitness.values\n",
        "              del child2.fitness.values\n",
        "      # mutation\n",
        "      for mutant in offspring:\n",
        "          if random.random() < MUTPB:\n",
        "              toolbox.mutate_task_order(mutant)\n",
        "              toolbox.mutate_processor_allocation(mutant)\n",
        "              toolbox.mutate_message_path_index(mutant)\n",
        "              toolbox.mutate_message_priority_ordering(mutant)\n",
        "              del mutant.fitness.values\n",
        "\n",
        "      # Evaluate the individuals with an invalid fitness\n",
        "      invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
        "      fitnesses = map(toolbox.evaluate, invalid_ind)\n",
        "      for ind, fit in zip(invalid_ind, fitnesses):\n",
        "          ind.fitness.values = fit\n",
        "      # The population is entirely replaced by the offspring\n",
        "      pop[:] = offspring\n",
        "  # Print the best individual\n",
        "  best_ind = tools.selBest(pop, 1)[0]\n",
        "  #print(\"Best individual is %s, %s\" % (best_ind, best_ind.fitness.values))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  task_order_len = num_tasks\n",
        "  processor_allocation_len = num_tasks\n",
        "  message_path_index_len = num_message\n",
        "  message_priority_ordering_len = num_message\n",
        "  task_order = best_ind[:task_order_len]\n",
        "  processor_allocation = best_ind[task_order_len:task_order_len + processor_allocation_len] #value encoding\n",
        "  message_path_index = best_ind[task_order_len + processor_allocation_len : task_order_len + processor_allocation_len + message_path_index_len] #value encoding\n",
        "  message_priority_ordering = best_ind[task_order_len + processor_allocation_len + message_path_index_len :] #permuation encoding\n",
        "  # print(\"task_order ================\", task_order)\n",
        "  # print(\"processor_allocation ================\", processor_allocation)\n",
        "  # print(\"message_path_index ================\", message_path_index)\n",
        "  # print(\"message_priority_ordering ================\", message_priority_ordering)\n",
        "  scheduleFinal, affected_list = reconstruct_schedule_with_precedenceXX(processor_allocation, task_order, processing_times, message_list, message_path_index, all_path_indexes_with_costs, message_priority_ordering,Data,contextList,Current_Context_event)      # Editing the re-construction (New 21-09-23)\n",
        "                                                                                                                                                                                                                                                  # 29.09.23 adding contextList,Current_Context_event as attributes\n",
        "  #print('The final schedule is {}'.format(scheduleFinal))\n",
        "  Final_genome= [task_order,processor_allocation,message_path_index,message_priority_ordering]\n",
        "  return scheduleFinal , Final_genome, affected_list\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LwzRZWnClbeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algorithm (1) 19-09-2023\n"
      ],
      "metadata": {
        "id": "8rl79o0_L3AT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "############################################################\n",
        "#Construct schedule to include context path index, context message ordering\n",
        "#Begin metascheduling algorithm implementation\n",
        "\n",
        "#number of context event n\n",
        "#n = len(contextList)-1                # The first elemnt in the list is excluded as it represents the parent schedule\n",
        "\n",
        "#print(\"the value of n = \", n)\n",
        "#contextList_old =[(0,0,0),(0,1,40),(0,2,50),(0,3,30),(0,4,60),(0,5,30)]#,(0,4,60),(0,5,30),(1,1,0),(1,2,0)] # Each index represesnts a specific context\n",
        "#contextList_old =[(0,0,0),(0,1,40),(0,2,50),(0,3,30),(0,4,60),(0,5,30),(0,12,40), (0,15,70), (0,18,60)] # 20 Jobs\n",
        "#contextList_old =[(0,0,0),(0,1,40),(0,2,50),(0,3,30),(0,4,60),(0,5,30)] # 20 Jobs\n",
        "contextList_old =[(0,0,0),(0,1,40),(0,2,50),(0,3,30),(0,4,60),(0,5,30),(0,12,40), (0,15,70), (0,18,60), (0,25,30),(0,35,60), (0,45,50)] # 80 Jobs\n",
        "\n",
        "\n",
        "\n",
        "#compute Base schedule\n",
        "SMbase, F_genome = generate_schedule_base_schedule(processing_times, message_list, all_path_indexes_with_costs)\n",
        "#index base schedule as root\n",
        "print(\"The Base Schedule Plot is :\")\n",
        "plot_schedule(SMbase)\n",
        "print('The base schedule makespan is {}'.format(compute_makespan(SMbase)))\n",
        "\n",
        "\n",
        "#Updating the Context List based on the base Schedule\n",
        "contextList = Update_Context_List(SMbase, contextList_old)            # 29.09.23\n",
        "print(\"The Updated Context list is : \", contextList)\n",
        "# Modified on Friday 15-09-2023 Omar\n",
        "AMbase = Read_Parent_AM(json_data)\n",
        "PMbase = Read_Parent_PM(json_data)\n",
        "\n",
        "# setSMx=SMbase;// whereSMx is predecessor schedule\n",
        "SMx = SMbase\n",
        "AMx = AMbase\n",
        "PMx = PMbase\n",
        "Initial_genome = F_genome\n",
        "c = 0                                             # Context event\n",
        "volatile_context_list = contextList               # contextList =[(0,0,0),(0,1,40),(0,2,50),(0,3,30)]\n",
        "Data = [(SMbase, AMbase, PMbase, 0,Initial_genome)]\n",
        "n=9\n",
        "def run_metascheduler(n):\n",
        "    bit_vector = [0] * n  # Initialize the bit vector with all zeros\n",
        "    #Database_Schedule_indices = []  # Initialize an empty list to store all bit vectors\n",
        "    iterate_nbit_vector_helper(bit_vector, n, 0,contextList,Data,SMx)\n",
        "\n",
        "\n",
        "# We loop through n-bit vector\n",
        "def iterate_nbit_vector_helper(bit_vector, n, index,contextList,Data,SMx):\n",
        "    if index == n:\n",
        "        if not (bit_vector == [0] * n):  # Exclude [0, 0, 0]\n",
        "            #print(bit_vector)  # Do something with the generated bit vector\n",
        "            Parent_Schedule_Bin,Parent_Schedule_dec = getparent(bit_vector)\n",
        "            #print(\"The parent schedule index is :\",Parent_Schedule_dec)\n",
        "            AMx,PMx = getPredecessors(bit_vector,Data)\n",
        "            AMy, PMy = Apply_Context(AMx,PMx,bit_vector,contextList)\n",
        "            p= bit_vector_to_value(bit_vector)\n",
        "            Current_Context_event = find_msb_position(bit_vector)      # 29.09.23 getting the index of the context event\n",
        "            print(\"The current context event is : \",Current_Context_event)\n",
        "            SMy,Genome, affected_list3 = GA_SOLVE(AMy,PMy,SMx,Data[Parent_Schedule_dec],contextList,Current_Context_event)  # Apply GA , Passing The Dataset as an argument (21-09-23) , Through it i will access the the final genome from GA\n",
        "                                                                                            # 29.09.23 Context List added in order to appear in the Re-construction scope\n",
        "                                                                                            # 29.09.23 Current_Context_event added to know what event should be applied in the reconstruction\n",
        "            schedule_makespan1 = compute_makespan(SMy)\n",
        "            #print(\"The makespan of this schedule is: \",compute_makespan(SMy))\n",
        "            #plot_schedule(SMy)\n",
        "\n",
        "            #p= bit_vector_to_value(bit_vector)\n",
        "            Schedule_Communication_Cost_value = len(affected_list3)*5\n",
        "            D=(SMy,AMy,PMy,p,Genome, Schedule_Communication_Cost_value,schedule_makespan1)\n",
        "            Data.append(D)\n",
        "            #print(\"The Data_base is : \" , Data)\n",
        "\n",
        "\n",
        "        return\n",
        "\n",
        "    # Recursively generate all possible combinations\n",
        "    bit_vector[index] = 0\n",
        "    iterate_nbit_vector_helper(bit_vector, n, index + 1,contextList,Data,SMx)\n",
        "\n",
        "    bit_vector[index] = 1\n",
        "    iterate_nbit_vector_helper(bit_vector, n, index + 1,contextList,Data,SMx)\n",
        "\n",
        "run_metascheduler(n)"
      ],
      "metadata": {
        "id": "2nnKVEhZG-jT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(Data))"
      ],
      "metadata": {
        "id": "kl558R7AyJtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## summing over the comm costs"
      ],
      "metadata": {
        "id": "9ex1MtMnjS1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(Data)"
      ],
      "metadata": {
        "id": "_I6gjWvTAw1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Data)\n",
        "print(len(Data))\n",
        "print(Data[31])\n",
        "print(Data[31][-2])\n",
        "print( len(contextList_old))\n",
        "print(2 ** (len(contextList_old)-1))"
      ],
      "metadata": {
        "id": "4ABhglpdjYdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Total_Cost= 0\n",
        "Avg_Cost = 0\n",
        "for i in range(1,(len(Data))):\n",
        "  Total_Cost += Data[i][-2]\n",
        "  Avg_Cost = Total_Cost/32\n",
        "\n",
        "print(Total_Cost)\n",
        "print(Avg_Cost)"
      ],
      "metadata": {
        "id": "um_e0XN1kJ7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Makespan Calcuation"
      ],
      "metadata": {
        "id": "MXAbkDnC9vnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "makespan_list= []\n",
        "for i in range(1,len(Data)):\n",
        "  #print(Data[i][-1])\n",
        "  makespan_list.append((Data[i][-1]))\n",
        "\n",
        "print(makespan_list)\n",
        "print(len(makespan_list))\n",
        "print(\"The Total makesspans is {}\".format(sum(makespan_list)+ compute_makespan(SMbase)))"
      ],
      "metadata": {
        "id": "H85YpbY1aYNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_makespan(SMbase)"
      ],
      "metadata": {
        "id": "lkmmpD53CBzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting Schedules"
      ],
      "metadata": {
        "id": "UZjOUyUjnAtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#len(Data)\n",
        "#print(Data[31][0])\n",
        "#plot_schedule(Data[31][0])\n",
        "#print(compute_makespan(Data[31][0]))\n",
        "\n",
        "for i in range(len(Data)):\n",
        "  plot_schedule(Data[i][0])\n",
        "  print(\"The makespan for schedule index {} is {}\".format((Data[i][-2]), compute_makespan(Data[i][0])))\n",
        "  #print(\"The makespan is {}\".format(compute_makespan(Data[i][0]))\n"
      ],
      "metadata": {
        "id": "1YXNCB6ELKw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Without factoring the communication cost**"
      ],
      "metadata": {
        "id": "8xgXVLs2dG3T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {
        "id": "WqNno4-Kdawz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Read_Parent_AM(json_data):     # Returning a dictionary about the Application graph\n",
        "  AMx = json_data['application']\n",
        "  #print(\"The Parent AM is \",AMx)\n",
        "  return AMx\n",
        "\n",
        "def Read_Parent_PM(json_data):     # Returning a dictionary about the Platform graph\n",
        "  PMx = json_data['platform']\n",
        "  return PMx\n",
        "\n",
        "\n",
        "def construct_communication_costs_from_json(json_data):\n",
        "  messages = json_data['application']['messages']\n",
        "  communication_costs = {}\n",
        "\n",
        "  for message in messages:\n",
        "      sender = message['sender']\n",
        "      receiver = message['receiver']\n",
        "      size = message['size']\n",
        "\n",
        "      if sender not in communication_costs:\n",
        "          communication_costs[sender] = {receiver: size}\n",
        "      else:\n",
        "          communication_costs[sender][receiver] = size\n",
        "\n",
        "  return communication_costs\n",
        "\n",
        "def construct_task_dag_from_json(APP_MODEL): # where APP_MODELis an instance from the function Read_Parent_AM(json_data)\n",
        "    # this function returns 2 lists one for task_dag (list of lists) for the successors\n",
        "    # Another list tis the wcet_values showing the worst excution times for each job\n",
        "    jobs = APP_MODEL['jobs']\n",
        "    messages = APP_MODEL['messages']\n",
        "\n",
        "    num_tasks = len(jobs)\n",
        "\n",
        "    # Create a mapping of sender and receiver tasks for each message\n",
        "    message_mapping = {}\n",
        "    for message in messages:\n",
        "        sender = message['sender']\n",
        "        receiver = message['receiver']\n",
        "        if sender not in message_mapping:\n",
        "            message_mapping[sender] = [receiver]\n",
        "        else:\n",
        "            message_mapping[sender].append(receiver)\n",
        "\n",
        "    # Create the task DAG\n",
        "    task_dag = [[] for _ in range(num_tasks)]\n",
        "\n",
        "    for job_id, successors in message_mapping.items():\n",
        "        task_dag[job_id] = successors\n",
        "\n",
        "    # Extract the WCET values\n",
        "    wcet_values = [job['processing_times'] for job in jobs]\n",
        "\n",
        "    return task_dag, wcet_values\n",
        "\n",
        "\n",
        "def extract_message_list(APP_MODEL):\n",
        "  # Returns a list of dictionaries for each meassage attribute where the keys for each message will be\n",
        "  # id,sender,reciever,size\n",
        "    messages = APP_MODEL['messages']                              # feteching messages (list of dictionaries)\n",
        "    task_ids = [job['id'] for job in APP_MODEL['jobs']]           # creating a list of task_ids\n",
        "    message_list = []                                                            # Initializing a list\n",
        "    for msg in messages:\n",
        "        sender_id = task_ids.index(msg['sender'])\n",
        "        receiver_id = task_ids.index(msg['receiver'])\n",
        "        message_size = msg['size']\n",
        "        message_id = msg['id']\n",
        "        message_info = {\n",
        "            'id': message_id,\n",
        "            'sender': sender_id,\n",
        "            'receiver': receiver_id,\n",
        "            'size': message_size\n",
        "        }\n",
        "        message_list.append(message_info)\n",
        "    return message_list\n",
        "\n",
        "def compute_makespan(schedule):    # passing the Re-construction function result\n",
        "    # Extract end times from the schedule\n",
        "    end_times = [info[2] for info in schedule.values()]\n",
        "    # The makespan is the maximum end time\n",
        "    makespan = max(end_times)\n",
        "    return makespan\n",
        "\n",
        "def plot_schedule(schedule):\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    processors = sorted(list(set([processor for processor, _, _ in schedule.values()])))\n",
        "    colors = plt.cm.tab10(np.linspace(0, 1, len(processors)))\n",
        "\n",
        "    for task, (processor, start_time, end_time) in schedule.items():\n",
        "        color = colors[processor]\n",
        "        ax.plot([start_time, end_time], [task, task], label=f'Processor {processor}', linewidth=10, marker='o', color=color)\n",
        "\n",
        "    ax.set_xlabel('Time')\n",
        "    ax.set_ylabel('Task')\n",
        "\n",
        "    # Calculate the makespan and set x-axis limit\n",
        "    makespan = max(end_time for _, (_, _, end_time) in schedule.items())\n",
        "    ax.set_xlim(0, makespan)\n",
        "\n",
        "    # Set y-axis ticks and labels\n",
        "    plt.yticks(range(len(schedule)))\n",
        "    plt.grid()\n",
        "    plt.title(\"Task Schedule\")\n",
        "\n",
        "    # Create a custom legend without duplicate labels\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    by_label = dict(zip(labels, handles))\n",
        "    ax.legend(by_label.values(), by_label.keys())\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def construct_graph_from_json(PLAT_MODEL):         # Function for constructing the PM , returns a graph object\n",
        "    # Extract nodes and links from JSON data\n",
        "    nodes = PLAT_MODEL['nodes']\n",
        "    links = PLAT_MODEL['links']\n",
        "\n",
        "    # Create an empty graph\n",
        "    graph = nx.Graph()\n",
        "\n",
        "    # Add nodes to the graph\n",
        "    for node in nodes:\n",
        "        node_id = node['id']\n",
        "        node_type = 'processor' if not node['is_router'] else 'switch'\n",
        "        graph.add_node(node_id, node_type=node_type)\n",
        "\n",
        "    # Add edges (links) to the graph\n",
        "    for link in links:\n",
        "        start = link['start']\n",
        "        end = link['end']\n",
        "        graph.add_edge(start, end)\n",
        "\n",
        "    return graph\n",
        "\n",
        "def generate_all_path_indexes_with_costs(graph):                           # Passing the PM as an argument , geneartes all paths between processors in the graph while calcualting the cost\n",
        "    processors = [node for node, data in graph.nodes(data=True) if data['node_type'] == 'processor']\n",
        "    # creating a list with the processor nodes ids\n",
        "    switches = [node for node, data in graph.nodes(data=True) if data['node_type'] == 'switch']\n",
        "    # creating a list with the switche nodes ids\n",
        "\n",
        "    def find_all_paths(source, target, path=[]): # A recursive function to find all paths between a give source node and target node, source and target are the iterables from the below for loop\n",
        "      # This function uses a depth first search (DFS) to explore the paths\n",
        "        path = path + [source]                    # Appending the \"source\" node to the path list, to keep track of nodes vistied in the list\n",
        "        if source == target:\n",
        "            return [path]\n",
        "        if source not in graph:                    # checks for node presence in graph\n",
        "            return []\n",
        "        paths = []\n",
        "        for node in graph[source]:                 # iterating through the current neighbors in graph\n",
        "            if node not in path:\n",
        "                newpaths = find_all_paths(node, target, path)  # calling the fuction recursively taking the neighbor node as a source node , target node is the same as the old one, path= path calculated so far\n",
        "                for newpath in newpaths:\n",
        "                    paths.append(newpath)\n",
        "        return paths\n",
        "\n",
        "    path_indexes = {}                             # Initialzing a dict. to store results (paths and costs)\n",
        "    path_id = 0\n",
        "    for source in processors:                     # iterating through the processors list in the PM [0,1,2]\n",
        "        for target in processors:                 # iterating through processors list in PM [0,1,2], done to consider all pairs between source and target nodes\n",
        "            if source != target:                  # checking if source and target nodes are not the same as we don't need a path from the processor to itself\n",
        "                all_paths = find_all_paths(source, target)\n",
        "                all_paths = [path for path in all_paths if any(node in path for node in switches)] # filtering the paths to keep the ones with only one switch node\n",
        "                if all_paths:\n",
        "                    for path in all_paths:\n",
        "                        # Compute the cost as the number of edges in the path\n",
        "                        path_cost = len(path) - 1                                   # Computing the cost by subtracting 1 from the number of nodes in the path.\n",
        "                        # Add the path, its ID, and its cost to the result\n",
        "                        path_indexes[path_id] = {\"path\": path, \"cost\": path_cost}\n",
        "                        path_id += 1\n",
        "    return path_indexes\n",
        "\n",
        "#Python function for n-bit\n",
        "def value_to_nbit_vector(value, n): # representing teh context event in bit form\n",
        "    binary = bin(value)[2:]  # Remove the '0b' prefix\n",
        "    binary = binary.zfill(n)  # Pad with leading zeros to make it n bits long\n",
        "    bit_vector = [int(bit) for bit in binary]  # Convert each character to integer, bit representation\n",
        "    return bit_vector\n",
        "\n",
        "#python function to return the highest bit set (To decode which context event we are dealing with. The value of c)\n",
        "def find_msb_position(bit_vector):\n",
        "    msb_position = 0\n",
        "    for i in range(len(bit_vector)):\n",
        "        if bit_vector[i] == 1:\n",
        "            msb_position = len(bit_vector) - i\n",
        "            break\n",
        "    return msb_position\n",
        "\n",
        "#function to convert bit_vector to integer\n",
        "def bit_vector_to_value(bit_vector):\n",
        "    value = 0\n",
        "    n = len(bit_vector)\n",
        "    for i in range(n):\n",
        "        value += bit_vector[i] * (2 ** (n - i - 1))\n",
        "    return value\n",
        "\n",
        "\n",
        "#This function is to find the equivalent binary value of c bit pattern\n",
        "def msb_only_bit_vector(bit_vector):\n",
        "    # Find the position of the most significant bit\n",
        "    msb_position = find_msb_position(bit_vector)\n",
        "\n",
        "    # Create a new bit vector with all bits set to 0\n",
        "    new_bit_vector = [0] * len(bit_vector)\n",
        "\n",
        "    # Set only the most significant bit to 1\n",
        "    if msb_position > 0:\n",
        "        new_bit_vector[-msb_position] = 1\n",
        "\n",
        "    return new_bit_vector\n",
        "\n",
        "def getparent(p):# where B is the Database indices and p is the schedule index\n",
        "  # Assuming p is integer\n",
        "  Schedule_index = p\n",
        "  #print('Schedule_index', Schedule_index)\n",
        "  Context_event_dec = find_msb_position(Schedule_index)\n",
        "  #print('Context_event_dec', Context_event_dec)\n",
        "\n",
        "\n",
        "  Context_event_Bin = msb_only_bit_vector(Schedule_index)\n",
        "  #print(\"Context_event_Bin\",Context_event_Bin)\n",
        "  Parent_Schedule_Bin = sub_list(Schedule_index,Context_event_Bin)\n",
        "  #print(\"Parent_Schedule_Bin\",Parent_Schedule_Bin)\n",
        "  Parent_Schedule_dec = bit_vector_to_value(Parent_Schedule_Bin)\n",
        "  #print(\"Parent_Schedule_dec\",Parent_Schedule_dec)\n",
        "  return Parent_Schedule_Bin,Parent_Schedule_dec\n",
        "\n",
        "def sub_list(list1, list2):                                                # Helper function called in getpredecessors(p)\n",
        "    if len(list1) != len(list2):\n",
        "        raise ValueError(\"Input lists must have the same length\")\n",
        "\n",
        "    result = [a - b for a, b in zip(list1, list2)]\n",
        "    return result\n",
        "\n",
        "def Apply_Context(AMx,PMx,p,contextList): # AMx= application model , PMx = Platform model, p = Parent Schedule index in binary\n",
        "  jobs = AMx['jobs']\n",
        "  #print(\"The p in the Apply_Context function is \",p)\n",
        "  context_event = find_msb_position(p)\n",
        "  #print(\"The context event  is :\",context_event)\n",
        "  for context in range(len(contextList)):\n",
        "    if context == context_event and contextList[context][0] == 0:\n",
        "      #print(\"The event {} is a SLACK!!\".format(contextList[context]))\n",
        "      for job in jobs:\n",
        "        #print(job)\n",
        "        if job[\"id\"] == contextList[context][1]:\n",
        "          job[\"processing_times\"]= int(round(job[\"processing_times\"]* (100-contextList[context][2])/100))\n",
        "\n",
        "\n",
        "          #print(\"The new AM is : \",AMx)\n",
        "    elif context == context_event and contextList[context][0] == 1:\n",
        "      print(\"The event {} is a Failure!!\".format(contextList[context]))\n",
        "  #print(\"Function Apply_Context successively applied!!\")\n",
        "  return AMx,PMx\n",
        "\n",
        "def getPredecessors(p,B): # where the first argument is Binary Encoding of the Schedule, and the second argumet is the Dataset\n",
        "  Parent_Schedule_Bin,Parent_Schedule_dec = getparent(p) # Calling the helper function to return the parent schedule as decimal and binary value\n",
        "  #print(\"The Parent Schedule of {} is {} in binary format and is {} in decimal format\".format(p,Parent_Schedule_Bin,Parent_Schedule_dec))   # print check\n",
        "  context_c = find_msb_position(Parent_Schedule_Bin)    # Getting the context event value of the parent schedule\n",
        "  #print(\"The context event of the Parent Schedule is : \",context_c)                      # print check\n",
        "  for i in range (len(B)): # iterating through the dataset\n",
        "    if B[i][3] == Parent_Schedule_dec:\n",
        "      AM,PM = B[i][1], B[i][2]\n",
        "      #print(\"Function getPredecessor successfuly deployed \")\n",
        "      #print(\"The AMx is {} and the PMx is {}\".format(AMx,PMx))\n",
        "\n",
        "      return AM,PM\n",
        "      #print(\"Function getPredecessor successfuly deployed \")\n",
        "  return None,None\n",
        "\n",
        "\n",
        "def GA_SOLVE(AMy,PMy,SMx,Data,contextList,Current_Context_event):                             # 29.09.23 ==> adding the Context List &  Current_Context_event attributes\n",
        "  successors, processing_times = construct_task_dag_from_json(AMy)\n",
        "  message_list = extract_message_list(AMy)\n",
        "  PLATFORM_GRAPH_OBJECT = construct_graph_from_json(PMy)\n",
        "  all_path_indexes_with_costs = generate_all_path_indexes_with_costs(PLATFORM_GRAPH_OBJECT)\n",
        "  n_tasks = len(processing_times)\n",
        "\n",
        "  #SMy = generate_schedule_base_schedule(processing_times, message_list, all_path_indexes_with_costs)  # INVOKING GA\n",
        "  SMy, Genome2 = generate_Successor_schedule2(processing_times, message_list, all_path_indexes_with_costs,Data,contextList,Current_Context_event)       # 29.09.23 Adding the context lIst & Current_Context_event attributes\n",
        "  return SMy , Genome2\n",
        "\n",
        "def Update_Context_List(Schedule, contextList):\n",
        "    for i in range(len(contextList)):\n",
        "        schedule_info = Schedule.get(i)  # Get the value of the specified key\n",
        "        if schedule_info:\n",
        "            start_time = schedule_info[1]\n",
        "            end_time = schedule_info[2]\n",
        "            context_time = int(round(start_time + (1 - (contextList[i][2] / 100)) * (end_time - start_time)))\n",
        "            contextList[i] = contextList[i] + (context_time,)\n",
        "    return contextList\n",
        "\n",
        "def Get_Scheduled_task(Data,contextList,Current_Context_event):\n",
        "  Scheduled_tasks = []\n",
        "  schedule= Data[0]\n",
        "  for context in range (len(contextList)):\n",
        "    if Current_Context_event == context:\n",
        "      #print(contextList[context])\n",
        "      current_context = contextList[context]\n",
        "      for task in schedule:\n",
        "        #print(task)\n",
        "        if schedule[task][2] <= current_context[-1]:\n",
        "          #print(schedule[task])\n",
        "          Scheduled_tasks.append(task)\n",
        "  return Scheduled_tasks\n",
        "\n",
        "def update_lists(Scheduled_tasks, node_list, task_allocation):\n",
        "    # Create a copy of the original lists to avoid modifying them directly\n",
        "    updated_node_list = node_list.copy()\n",
        "    updated_task_allocation = task_allocation.copy()\n",
        "\n",
        "    for task in Scheduled_tasks:\n",
        "        while task in updated_node_list:\n",
        "            # Exclude all occurrences of the task from the node_list\n",
        "            index = updated_node_list.index(task)\n",
        "            updated_node_list.pop(index)\n",
        "\n",
        "            # Exclude the corresponding processor\n",
        "            updated_task_allocation.pop(index)\n",
        "\n",
        "    return updated_node_list, updated_task_allocation\n",
        "\n",
        "def filter_scheduled_tasks(Data, Scheduled_tasks):\n",
        "    schedule = Data[0]\n",
        "    scheduled_dict = {}\n",
        "\n",
        "    for task_id in Scheduled_tasks:\n",
        "        if task_id in schedule:\n",
        "            scheduled_dict[task_id] = schedule[task_id]\n",
        "\n",
        "    return scheduled_dict\n",
        "\n",
        "# TEST FUNCTIONS\n",
        "\n",
        "#AMx = Read_Parent_AM(json_data)\n",
        "#PMx = Read_Parent_PM(json_data)\n",
        "##print(AMx)\n",
        "#print(PMx)\n",
        "#type(AMx)\n",
        "#successors, processing_times = construct_task_dag_from_json(AMx)\n",
        "#print(successors)\n",
        "#print(processing_times)\n",
        "#message_list = extract_message_list(AMx)\n",
        "#print(message_list)\n",
        "#PLATFORM_GRAPH_OBJECT = construct_graph_from_json(PMx)\n",
        "#print(PLATFORM_GRAPH_OBJECT)\n",
        "#all_path_indexes_with_costs = generate_all_path_indexes_with_costs(PLATFORM_GRAPH_OBJECT)\n",
        "#print(all_path_indexes_with_costs)"
      ],
      "metadata": {
        "id": "E_W2y7mkdL6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function Instances **(Don't run again !!!!!!!!)**"
      ],
      "metadata": {
        "id": "4oo6MyTediHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AMx = Read_Parent_AM(json_data)\n",
        "PMx = Read_Parent_PM(json_data)\n",
        "successors, processing_times = construct_task_dag_from_json(AMx)\n",
        "message_list = extract_message_list(AMx)\n",
        "PLATFORM_GRAPH_OBJECT = construct_graph_from_json(PMx)\n",
        "all_path_indexes_with_costs = generate_all_path_indexes_with_costs(PLATFORM_GRAPH_OBJECT)\n",
        "n_tasks = len(processing_times)"
      ],
      "metadata": {
        "id": "DejgAa2heabi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algorithm 3"
      ],
      "metadata": {
        "id": "7DRNkW60eWAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reconstruct_schedule_with_precedenceX(task_allocation, node_list, processing_times, message_list, message_path_index, all_path_indexes_with_costs, message_priority_ordering):\n",
        "    # Create a deep copy of the message_list to avoid modifying the original data\n",
        "    message_list_copy = deepcopy(message_list)            #creating a copy of the message list\n",
        "\n",
        "    num_processors = max(task_allocation) + 1             # specifing the number of processors\n",
        "    processors = [[] for _ in range(num_processors)]      # creating a list of empty lists of size = processors\n",
        "\n",
        "    for task, processor in enumerate(task_allocation):    # Appending the list of processor\n",
        "        processors[processor].append(task)\n",
        "\n",
        "    schedule = {}                                         # Initializing a dictionary for the new schedule.\n",
        "    task_completion_times = [0] * len(node_list)          # Initializing a list of zeros with size = node list\n",
        "    message_dict = defaultdict(list)                      # creating an instance of defaultdict, it allows define a default value for keys that do not yet exist in the dictionary.\n",
        "    for i in range(n_tasks):\n",
        "        message_dict[i] = []                              # Initializing with all potential receivers\n",
        "\n",
        "\n",
        "    # Create a dictionary to map message ID to priority\n",
        "    #message_priority_dict = {idx: priority for priority, idx in enumerate(message_priority_ordering)}\n",
        "    # Modify the message_priority_dict to use actual message IDs\n",
        "    message_priority_dict = {message_id: priority for priority, message_id in enumerate(message_priority_ordering)}       # Mapping messages to priorities\n",
        "\n",
        "\n",
        "    for idx, message in enumerate(message_list_copy):\n",
        "        # Decode the message path index to the corresponding path index\n",
        "        path_id = message_path_index[idx]\n",
        "        path = all_path_indexes_with_costs[path_id][\"path\"]\n",
        "\n",
        "        path_cost= all_path_indexes_with_costs[path_id][\"cost\"]\n",
        "\n",
        "        # Adjust the size of the message with the cost of the path\n",
        "        message[\"size\"] += path_cost\n",
        "\n",
        "        # Append a tuple containing the sender, message size, and message priority to the receiver's list in the message_dict\n",
        "        #message_dict[message[\"receiver\"]].append((message[\"sender\"], message[\"size\"], message_priority_dict[idx]))\n",
        "\n",
        "        # Use message[\"id\"] to get the priority from message_priority_dict\n",
        "        message_dict[message[\"receiver\"]].append((message[\"sender\"], message[\"size\"], message_priority_dict[message[\"id\"]]))\n",
        "\n",
        "    # Sort each receiver's list in message_dict by message priority\n",
        "    for receiver, messages in message_dict.items():\n",
        "        message_dict[receiver] = sorted(messages, key=lambda x: x[2])\n",
        "\n",
        "    current_time_per_processor = [0] * num_processors                 # initilaizing a list of zeros of size = No. of processors\n",
        "    completed_tasks = set()                                           # creating an empty set for the completed tasks\n",
        "    ready_tasks = set(range(len(node_list)))                          # A set of random numbers of size = length of node list\n",
        "\n",
        "    while ready_tasks:                                                # iterating through the set of tasks\n",
        "        task = ready_tasks.pop()                                      # Acessing a task randomly and removing it from ready tasks set\n",
        "        task_id = node_list[task]                                     # getting the task id\n",
        "        processor = task_allocation[task]                             # Acessing the processor on which task should run\n",
        "        i = processor                                                 # Assigning the value of processor to i\n",
        "\n",
        "        predecessors = message_dict[task_id]                          # returns a tuple of [(sender,size,id)]\n",
        "        if all(p in completed_tasks for p, _, _ in predecessors):     # Checks if all pre-decessors(sender in the tuple of predecessors) are in completed tasks set\n",
        "            if predecessors:\n",
        "                # Ensure that the start_time is at least the end_time of the last task on this processor\n",
        "                start_time = max(current_time_per_processor[i], max(task_completion_times[sender] + size for sender, size, _ in predecessors))\n",
        "            else:\n",
        "                start_time = current_time_per_processor[i]\n",
        "\n",
        "            end_time = start_time + processing_times[task_id]\n",
        "            schedule[task_id] = (i, start_time, end_time)\n",
        "            task_completion_times[task_id] = end_time\n",
        "\n",
        "            current_time_per_processor[i] = end_time\n",
        "            completed_tasks.add(task_id)\n",
        "        else:\n",
        "            ready_tasks.add(task)\n",
        "\n",
        "    return schedule"
      ],
      "metadata": {
        "id": "oQopW10DfW6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Base schedule (ALG 2)"
      ],
      "metadata": {
        "id": "lqZuOikGfblN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_schedule_base_schedule(processing_times, message_list, all_path_indexes_with_costs):\n",
        "\n",
        "  creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))     # creating a FitnessMin class for minimization and setting the weights to (-1.0,).\n",
        "  creator.create(\"Individual\", list, fitness=creator.FitnessMin)  # creating the Individual class as a list with the FitnessMin class as the fitness attribute.\n",
        "\n",
        "  toolbox = base.Toolbox()\n",
        "\n",
        "  #parameters\n",
        "  num_tasks = len(processing_times)                               # No of jobs in task graph\n",
        "  nodes = json_data[\"platform\"][\"nodes\"]                          # Accessing the nodes of the Platform model\n",
        "  #processors = [node for node in nodes if not node[\"is_router\"]]  # Creating  a list of the processors in the Platform model.\n",
        "  num_machines =2                               # A variable specifing the number of machines\n",
        "  num_message = len(message_list)                                 # A varible = No. of messages\n",
        "  num_paths = len(all_path_indexes_with_costs)                    # total number of paths\n",
        "\n",
        "  # Define the initialization function for the individual\n",
        "  def init_individual():\n",
        "      return random.sample(range(num_tasks), num_tasks)           # returns a list of random numbers in the range of num_tasks\n",
        "\n",
        "  #print('The initial Indidvidual is {}'.format(init_individual()))  # OMAR\n",
        "\n",
        "  # Register the initialization function in the DEAP toolbox\n",
        "  toolbox.register(\"task_order\", init_individual)\n",
        "\n",
        "  #toolbox.register(\"task_order\", tools.initIterate, creator.Individual, init_individual)\n",
        "\n",
        "  #toolbox.register(\"task_order\", random.sample, range(num_tasks), num_tasks)\n",
        "\n",
        "  # Processor_allocation value encoding\n",
        "  def processor_allocation(n_task, n_machines):\n",
        "      return [random.randint(0, num_machines) for _ in range(n_task)]   # A random list , contains the allocation of each job with the corresponding task\n",
        "  toolbox.register(\"processor_allocation\", processor_allocation, n_task=num_tasks, n_machines=num_machines)\n",
        "  HH = toolbox.processor_allocation()                                                     # OMAR\n",
        "  #print('The randomly initialized task allocation looks as the follows {}'.format(HH))    # OMAR\n",
        "\n",
        "  # Message_path_index value encoding\n",
        "  def message_path_index(n_messages, max_all_path_index):\n",
        "      return [random.randint(0, max_all_path_index - 1) for _ in range(n_messages)]  # Ensure indices are within range\n",
        "      # returns a list of random numbers of size = n_messages and values ranging from (0, max_all_path_index - 1).\n",
        "\n",
        "  toolbox.register(\"message_path_index\", message_path_index, n_messages=num_message, max_all_path_index= num_paths)\n",
        "\n",
        "  #print('The randomly initialized message path looks as the follows {}'.format(toolbox.message_path_index()))    # OMAR\n",
        "\n",
        "\n",
        "  # Message_priority_ordering permutation encoding\n",
        "  def message_priority_ordering(n_messages):\n",
        "      return random.sample(range(n_messages), n_messages)         # returns a list of size = n_messages, showing the messages ordering\n",
        "  toolbox.register(\"message_priority_ordering\", message_priority_ordering, n_messages=num_message)\n",
        "\n",
        "  #print('The random priority ordering looks as follows {}'.format(toolbox.message_priority_ordering()))        # OMAR\n",
        "\n",
        "  # Combined Individual\n",
        "  def create_individual():\n",
        "      individual = []\n",
        "      individual.extend(toolbox.task_order())\n",
        "      individual.extend(toolbox.processor_allocation())\n",
        "      individual.extend(toolbox.message_path_index())\n",
        "      individual.extend(toolbox.message_priority_ordering())\n",
        "      return individual\n",
        "  # To view the whole indivudual\n",
        "  #individual = create_individual()\n",
        "  #print(\"Individual:\", individual)\n",
        "  toolbox.register(\"individual\", tools.initIterate, creator.Individual, create_individual)\n",
        "\n",
        "  #print('The initial combined individual is {}'.format(toolbox.individual()))                  # OMAR\n",
        "\n",
        "  # Population initialization\n",
        "  toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "  #print('The initial population is {}'.format(toolbox.population(n=100)))\n",
        "\n",
        "  def reconstruct_schedule(processor_allocation, task_order):\n",
        "      # Initialize the schedule as a dictionary of empty lists\n",
        "      # where each key is a processor id\n",
        "      schedule = {i: [] for i in range(max(processor_allocation) + 1)}\n",
        "\n",
        "      # Iterate over tasks in their given order\n",
        "      for task in task_order:\n",
        "          # Assign each task to its processor\n",
        "          schedule[processor_allocation[task]].append(task)\n",
        "\n",
        "\n",
        "\n",
        "      return schedule\n",
        "  #print ('The Initial Schedule looks as {}'.format(reconstruct_schedule(toolbox.processor_allocation(), toolbox.task_order())))     # OMAR\n",
        "\n",
        "  ############################################################################################################################################################\n",
        "\n",
        "  ####################################################### Evaluation, selection, mating, and mutation methods ################################################\n",
        "\n",
        "  #########compute makespan function ################################################\n",
        "  def compute_makespan(schedule):\n",
        "      # Extract end times from the schedule\n",
        "      end_times = [info[2] for info in schedule.values()]\n",
        "      # The makespan is the maximum end time\n",
        "      makespan = max(end_times)\n",
        "      return makespan\n",
        "  ################################################################\n",
        "\n",
        "  # Define the evaluation function\n",
        "  def evaluate(individual, processing_times, message_list, all_path_indexes_with_costs):\n",
        "      # Split individual into its components\n",
        "      task_order_len = num_tasks                  # An Integer with the number of the tasks\n",
        "      processor_allocation_len = num_tasks        # Integer\n",
        "      message_path_index_len = num_message        # Integer\n",
        "      message_priority_ordering_len = num_message # Integer\n",
        "      task_order = individual[:task_order_len]    # permuation encoding, Acessing the first portion of the chromosome\n",
        "      processor_allocation = individual[task_order_len:task_order_len + processor_allocation_len] # value encoding acessing the second portion of chromosomes\n",
        "      message_path_index = individual[task_order_len + processor_allocation_len : task_order_len + processor_allocation_len + message_path_index_len] # value encoding\n",
        "      message_priority_ordering = individual[task_order_len + processor_allocation_len + message_path_index_len :] # permuation encoding\n",
        "      # print(\"task_order ================\", task_order)\n",
        "      # print(\"processor_allocation ================\", processor_allocation)\n",
        "      # print(\"message_path_index ================\", message_path_index)\n",
        "      # print(\"message_priority_ordering ================\", message_priority_ordering)\n",
        "      schedule = reconstruct_schedule_with_precedenceX(processor_allocation, task_order, processing_times, message_list, message_path_index, all_path_indexes_with_costs, message_priority_ordering)\n",
        "      makespan  = compute_makespan(schedule)\n",
        "      fitness = 1.0 / (makespan)\n",
        "      #print(\"Makespan over time is %s, %s\", makespan)\n",
        "      #scheduleR = reconstruct_schedule(processor_allocation, task_order)\n",
        "      # You would need to split the individual into its parts and use these parts to calculate the makespan\n",
        "      # For the sake of this example, let's just return the sum of all elements in the individual\n",
        "      return fitness,\n",
        "\n",
        "  # Register the evaluation function\n",
        "  toolbox.register(\"evaluate\", evaluate,processing_times=processing_times,\n",
        "                  message_list=message_list, all_path_indexes_with_costs=all_path_indexes_with_costs)\n",
        "\n",
        "  # Register selection operator\n",
        "  toolbox.register(\"select\", tools.selTournament, tournsize=3) # Applying tournament selection, tournsize=3\n",
        "                                                               # meaning three individuals will be randomly selected from the population,\n",
        "                                                               # and the best one among them will be chosen as a parent for the next generation.\n",
        "\n",
        "\n",
        "  # Register the crossover operator (Permutation encoded)\n",
        "  def mate(ind1, ind2, task_order_len):\n",
        "      # Copy the individuals\n",
        "      child1, child2 = toolbox.clone(ind1), toolbox.clone(ind2) # creating copies from ind1 and ind2 to preserve original originals while crossover and mutation.\n",
        "\n",
        "      # Apply PMX crossover to task_order\n",
        "      tools.cxPartialyMatched(child1[:task_order_len], child2[:task_order_len]) # Applying partially matched crossover method\n",
        "\n",
        "      # Return the modified individuals\n",
        "      return child1, child2\n",
        "\n",
        "  toolbox.register(\"mate\", mate, task_order_len=num_tasks)                # Register the crossover function\n",
        "\n",
        "\n",
        "  #toolbox.register(\"mate\", tools.cxTwoPoint)\n",
        "\n",
        "\n",
        "  # mutation operators\n",
        "  # def mutation_task_order(individual, task_order_len):\n",
        "  #     task_order = individual[:task_order_len] # permutation encoding\n",
        "  #     random.shuffle(task_order) # Shuffle the indexes of task_order\n",
        "  #     individual[:task_order_len] = task_order\n",
        "  #     return individual,\n",
        "  def mutation_task_order(individual, task_order_len):\n",
        "      task_order = individual[:task_order_len]  # permutation encoding\n",
        "      indices = random.sample(range(task_order_len), 2)  # Select two random indices\n",
        "      task_order[indices[0]], task_order[indices[1]] = task_order[indices[1]], task_order[indices[0]]  # Swap the elements at the selected indices\n",
        "      individual[:task_order_len] = task_order\n",
        "      return individual,\n",
        "\n",
        "  def mutation_processor_allocation(individual,task_order_len, processor_allocation_len): # (task_order_len = processor_allocation_len)\n",
        "      processor_allocation = individual[task_order_len:task_order_len + processor_allocation_len] # value encoding\n",
        "      tools.mutUniformInt(processor_allocation, low=0, up=1, indpb=0.05) # Mutate processor_allocation by uniformly changing its elements with an indpb chance\n",
        "      individual[task_order_len:task_order_len + processor_allocation_len] = processor_allocation\n",
        "      return individual,\n",
        "\n",
        "  def mutation_message_path_index(individual, task_order_len, processor_allocation_len, message_path_index_len):\n",
        "      message_path_index = individual[task_order_len + processor_allocation_len : task_order_len + processor_allocation_len + message_path_index_len] # value encoding\n",
        "      tools.mutUniformInt(message_path_index, low=0, up=1, indpb=0.05) # Mutate message_path_index by uniformly changing its elements with an indpb chance\n",
        "      individual[task_order_len + processor_allocation_len : task_order_len + processor_allocation_len + message_path_index_len] = message_path_index\n",
        "      return individual,\n",
        "\n",
        "  def mutation_message_priority_ordering(individual, task_order_len, processor_allocation_len, message_path_index_len):\n",
        "      message_priority_ordering = individual[task_order_len + processor_allocation_len + message_path_index_len :] # permuation encoding\n",
        "      tools.mutShuffleIndexes(message_priority_ordering, indpb=0.05) # Mutate message_priority_ordering by shuffling its indexes with an indpb chance\n",
        "      individual[task_order_len + processor_allocation_len + message_path_index_len :] = message_priority_ordering\n",
        "      return individual,\n",
        "\n",
        "  # register the mutation operators\n",
        "  toolbox.register(\"mutate_task_order\", mutation_task_order, task_order_len = num_tasks)\n",
        "  toolbox.register(\"mutate_processor_allocation\", mutation_processor_allocation, task_order_len = num_tasks, processor_allocation_len = num_tasks)\n",
        "  toolbox.register(\"mutate_message_path_index\", mutation_message_path_index, task_order_len = num_tasks, processor_allocation_len = num_tasks, message_path_index_len = num_message)\n",
        "  toolbox.register(\"mutate_message_priority_ordering\", mutation_message_priority_ordering, task_order_len = num_tasks, processor_allocation_len = num_tasks, message_path_index_len = num_message)\n",
        "\n",
        "  ######################################################\n",
        "  # Create an initial population\n",
        "  pop = toolbox.population(n=100)\n",
        "  # Evaluate the entire population\n",
        "  fitnesses = map(toolbox.evaluate, pop)\n",
        "  for ind, fit in zip(pop, fitnesses):\n",
        "      ind.fitness.values = fit\n",
        "  # Crossover probability and mutation probability\n",
        "  CXPB, MUTPB = 0.3, 0.2\n",
        "\n",
        "  # Extract all the fitnesses of\n",
        "  fits = [ind.fitness.values[0] for ind in pop]\n",
        "\n",
        "  NGEN = 50 #Number of generations\n",
        "  # Begin the evolution\n",
        "  for g in range(NGEN):\n",
        "      # A new generation\n",
        "      offspring = toolbox.select(pop, len(pop)) # Apply the tournmnet selection , 3 individulas will be selected\n",
        "      # Clone the selected individuals\n",
        "      offspring = list(map(toolbox.clone, offspring))\n",
        "\n",
        "      # Apply crossover and mutation on the offspring\n",
        "      for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
        "          if random.random() < CXPB:\n",
        "              toolbox.mate(child1, child2) # Applying crossover for Permutation encoding (task orders)\n",
        "              del child1.fitness.values\n",
        "              del child2.fitness.values\n",
        "      # mutation\n",
        "      for mutant in offspring:\n",
        "          if random.random() < MUTPB:\n",
        "              toolbox.mutate_task_order(mutant)\n",
        "              toolbox.mutate_processor_allocation(mutant)\n",
        "              toolbox.mutate_message_path_index(mutant)\n",
        "              toolbox.mutate_message_priority_ordering(mutant)\n",
        "              del mutant.fitness.values\n",
        "\n",
        "      # Evaluate the individuals with an invalid fitness\n",
        "      invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
        "      fitnesses = map(toolbox.evaluate, invalid_ind)\n",
        "      for ind, fit in zip(invalid_ind, fitnesses):\n",
        "          ind.fitness.values = fit\n",
        "      # The population is entirely replaced by the offspring\n",
        "      pop[:] = offspring\n",
        "  # Print the best individual\n",
        "  best_ind = tools.selBest(pop, 1)[0]\n",
        "  #print(\"Best individual is %s, %s\" % (best_ind, best_ind.fitness.values))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  task_order_len = num_tasks\n",
        "  processor_allocation_len = num_tasks\n",
        "  message_path_index_len = num_message\n",
        "  message_priority_ordering_len = num_message\n",
        "  task_order = best_ind[:task_order_len]\n",
        "  processor_allocation = best_ind[task_order_len:task_order_len + processor_allocation_len] #value encoding\n",
        "  message_path_index = best_ind[task_order_len + processor_allocation_len : task_order_len + processor_allocation_len + message_path_index_len] #value encoding\n",
        "  message_priority_ordering = best_ind[task_order_len + processor_allocation_len + message_path_index_len :] #permuation encoding\n",
        "  # print(\"task_order ================\", task_order)\n",
        "  # print(\"processor_allocation ================\", processor_allocation)\n",
        "  # print(\"message_path_index ================\", message_path_index)\n",
        "  # print(\"message_priority_ordering ================\", message_priority_ordering)\n",
        "  scheduleFinal = reconstruct_schedule_with_precedenceX(processor_allocation, task_order, processing_times, message_list, message_path_index, all_path_indexes_with_costs, message_priority_ordering)\n",
        "  #print('The final schedule is {}'.format(scheduleFinal))\n",
        "  Final_genome= [task_order,processor_allocation,message_path_index,message_priority_ordering]\n",
        "  return scheduleFinal , Final_genome\n",
        "\n",
        "\n",
        "\n",
        "#scheduleFinal, Final_genome= generate_schedule_base_schedule(processing_times, message_list, all_path_indexes_with_costs)\n",
        "\n",
        "#makespanFinal  = compute_makespan(scheduleFinal)\n",
        "\n",
        "#plot_schedule(scheduleFinal)\n",
        "#print('The final schedule makespan is {}'.format(makespanFinal))\n",
        "#print(\"The Final Genome for this schdule is {}\".format(Final_genome))"
      ],
      "metadata": {
        "id": "psy6NEXyf2cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reconstruction 2"
      ],
      "metadata": {
        "id": "gqsAs4-mf_U9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reconstruct_schedule_with_precedenceXX(task_allocation, node_list, processing_times, message_list, message_path_index, all_path_indexes_with_costs, message_priority_ordering,Data,contextList,Current_Context_event): # New 21-09-23\n",
        "  # Data ==> has only information about the previous schedule\n",
        "  # Create a deep copy of the message_list to avoid modifying the original data\n",
        "    message_list_copy = deepcopy(message_list)            #creating a copy of the message list\n",
        "\n",
        "    #print(\"The previous Data is : \",Data)\n",
        "\n",
        "\n",
        "\n",
        "   # Call Affected_ES function to get the list of affected processors\n",
        "    #affected_processors = Affected_ES(Data, node_list, task_allocation, message_path_index, message_priority_ordering)\n",
        "\n",
        "\n",
        "    # New Modifications (1) 29-09-23\n",
        "    Scheduled_tasks = Get_Scheduled_task(Data,contextList,Current_Context_event)   # Returns a list of tasks that where previously scheduled\n",
        "    #print(\"The Scheduled Tasks are\",Scheduled_tasks )\n",
        "\n",
        "    # New Modification (2) 29-09-23\n",
        "    new_node_list , new_task_allocation = update_lists(Scheduled_tasks, node_list, task_allocation)\n",
        "    #print(\"The node_list after update is: \",new_node_list)\n",
        "    #print(\"The Task_allocation after update is:\", new_task_allocation)\n",
        "\n",
        "    # New Modification (3) 29.09.23\n",
        "    Partial_Schedule = filter_scheduled_tasks(Data, Scheduled_tasks)\n",
        "    #print(\"The unchanged part from the parent schedule is :\", Partial_Schedule)\n",
        "\n",
        "    num_processors = max(task_allocation) + 1             # specifing the number of processors\n",
        "    processors = [[] for _ in range(num_processors)]      # creating a list of empty lists of size = processors\n",
        "\n",
        "    for task, processor in enumerate(new_task_allocation):    # Appending the list of processor\n",
        "        processors[processor].append(task)\n",
        "\n",
        "    n_tasks = len(new_node_list)\n",
        "    schedule = {}                                         # Initializing a dictionary for the new schedule.\n",
        "    task_completion_times = [0] * len(node_list)          # Initializing a list of zeros with size = node list\n",
        "    message_dict = defaultdict(list)                      # creating an instance of defaultdict, it allows define a default value for keys that do not yet exist in the dictionary.\n",
        "    for i in range(n_tasks):\n",
        "        message_dict[i] = []                              # Initializing with all potential receivers\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Create a dictionary to map message ID to priority\n",
        "    #message_priority_dict = {idx: priority for priority, idx in enumerate(message_priority_ordering)}\n",
        "    # Modify the message_priority_dict to use actual message IDs\n",
        "    message_priority_dict = {message_id: priority for priority, message_id in enumerate(message_priority_ordering)}       # Mapping messages to priorities\n",
        "\n",
        "\n",
        "    for idx, message in enumerate(message_list_copy):\n",
        "        # Decode the message path index to the corresponding path index\n",
        "        path_id = message_path_index[idx]\n",
        "        path = all_path_indexes_with_costs[path_id][\"path\"]\n",
        "\n",
        "        path_cost= all_path_indexes_with_costs[path_id][\"cost\"]\n",
        "\n",
        "        # Adjust the size of the message with the cost of the path\n",
        "        message[\"size\"] += path_cost\n",
        "\n",
        "        # Append a tuple containing the sender, message size, and message priority to the receiver's list in the message_dict\n",
        "        #message_dict[message[\"receiver\"]].append((message[\"sender\"], message[\"size\"], message_priority_dict[idx]))\n",
        "\n",
        "        # Use message[\"id\"] to get the priority from message_priority_dict\n",
        "        message_dict[message[\"receiver\"]].append((message[\"sender\"], message[\"size\"], message_priority_dict[message[\"id\"]]))\n",
        "\n",
        "    # Sort each receiver's list in message_dict by message priority\n",
        "    for receiver, messages in message_dict.items():\n",
        "        message_dict[receiver] = sorted(messages, key=lambda x: x[2])\n",
        "\n",
        "    current_time_per_processor = [contextList[Current_Context_event][-1]] * num_processors    # 29.09.23 Updating the start time based on the context event time\n",
        "    #print(\"The current time per processor list \",current_time_per_processor)\n",
        "    completed_tasks = set()                                           # creating an empty set for the completed tasks\n",
        "    completed_tasks = set(Partial_Schedule.keys())                    # 09.10.2023 edited\n",
        "\n",
        "    ready_tasks = set(range(len(new_node_list)))                          # A set of random numbers of size = length of node list\n",
        "    #print(\"Ready tasks are\",ready_tasks)\n",
        "\n",
        "\n",
        "    while ready_tasks:                                                # iterating through the set of tasks\n",
        "        task = ready_tasks.pop()                                      # Acessing a task randomly and removing it from ready tasks set\n",
        "        #print(\"The Task in read_tasks look is :\",task)\n",
        "        task_id = new_node_list[task]                                     # getting the task id\n",
        "        #print(\"The task_id in ready_tasks loop is\",task_id)\n",
        "        processor = new_task_allocation[task]                             # Acessing the processor on which task should run\n",
        "        #print(\"The corresponding processor is :\",processor)\n",
        "        i = processor                                                 # Assigning the value of processor to i\n",
        "\n",
        "        # Introduce fixed delay for affected processors\n",
        "        #if processor in affected_processors:\n",
        "            #current_time_per_processor[processor] += 5\n",
        "        #print(\"The processor times are :\",current_time_per_processor)\n",
        "\n",
        "        predecessors = message_dict[task_id]                          # returns a tuple of [(sender,size,id)]\n",
        "        #print(\"The predecessors are \",predecessors)\n",
        "        if all(p in completed_tasks for p, _, _ in predecessors):     # Checks if all pre-decessors(sender in the tuple of predecessors) are in completed tasks set\n",
        "            #print(\"ïnside the if \")\n",
        "            if predecessors:\n",
        "                # Ensure that the start_time is at least the end_time of the last task on this processor\n",
        "                start_time = max(current_time_per_processor[i], max(task_completion_times[sender] + size for sender, size, _ in predecessors))\n",
        "                #print(\"The start_time is :\",start_time)\n",
        "            else:\n",
        "                start_time = current_time_per_processor[i]\n",
        "                #print(\"The start time in else is :\",start_time)\n",
        "\n",
        "            end_time = start_time + processing_times[task_id]\n",
        "            #print(\"The endtime is : \",end_time)\n",
        "            schedule[task_id] = (i, start_time, end_time)\n",
        "            task_completion_times[task_id] = end_time\n",
        "\n",
        "            current_time_per_processor[i] = end_time\n",
        "            completed_tasks.add(task_id)\n",
        "        else:\n",
        "            ready_tasks.add(task)\n",
        "    #print(\"The Previous genome is : \",Data[-1])\n",
        "\n",
        "    merged_schedule = deepcopy(Partial_Schedule)  # creating  a copy of Partial_Schedule (predecessor)\n",
        "    merged_schedule.update(schedule)\n",
        "\n",
        "    return merged_schedule\n",
        "\n",
        "\n",
        "# Test\n",
        "#task_allocation= [1, 0, 1, 1, 0, 0, 2, 2, 2, 2, 0]\n",
        "#node_list =      [9, 3, 7, 1, 0, 5, 4, 10, 8, 2, 6]\n",
        "#message_path_index = [0, 1, 0, 1, 1, 9, 1, 4, 1, 6, 1, 3, 0, 9]\n",
        "#message_priority_ordering = [2, 9, 1, 5, 13, 6, 7, 12, 11, 3, 10, 4, 8, 0]\n",
        "#Data = ({0: (0, 0, 2), 1: (1, 11, 15), 5: (1, 22, 25), 9: (1, 32, 35), 3: (1, 35, 40), 4: (1, 40, 44), 8: (1, 51, 55), 7: (1, 55, 60), 2: (1, 60, 64), 6: (1, 70, 72), 10: (1, 78, 80)}, {'jobs': [{'id': 0, 'processing_times': 2, 'mcet': 10, 'deadline': 250, 'can_run_on': [0, 1, 2]}, {'id': 1, 'processing_times': 2, 'mcet': 10, 'deadline': 250, 'can_run_on': [0, 1, 2]}, {'id': 2, 'processing_times': 4, 'mcet': 10, 'deadline': 300, 'can_run_on': [0, 1, 2]}, {'id': 3, 'processing_times': 5, 'mcet': 10, 'deadline': 256, 'can_run_on': [0, 1, 2]}, {'id': 4, 'processing_times': 4, 'mcet': 10, 'deadline': 256, 'can_run_on': [0, 1, 2]}, {'id': 5, 'processing_times': 3, 'mcet': 10, 'deadline': 256, 'can_run_on': [0, 1, 2]}, {'id': 6, 'processing_times': 2, 'mcet': 10, 'deadline': 256, 'can_run_on': [0, 1, 2]}, {'id': 7, 'processing_times': 5, 'mcet': 10, 'deadline': 256, 'can_run_on': [0, 1, 2]}, {'id': 8, 'processing_times': 4, 'mcet': 10, 'deadline': 256, 'can_run_on': [0, 1, 2]}, {'id': 9, 'processing_times': 3, 'mcet': 10, 'deadline': 256, 'can_run_on': [0, 1, 2]}, {'id': 10, 'processing_times': 2, 'mcet': 10, 'deadline': 256, 'can_run_on': [0, 1, 2]}], 'messages': [{'id': 0, 'sender': 0, 'receiver': 1, 'size': 6, 'timetriggered': True}, {'id': 1, 'sender': 0, 'receiver': 2, 'size': 2, 'timetriggered': True}, {'id': 2, 'sender': 0, 'receiver': 3, 'size': 3, 'timetriggered': True}, {'id': 3, 'sender': 0, 'receiver': 4, 'size': 4, 'timetriggered': True}, {'id': 4, 'sender': 1, 'receiver': 5, 'size': 4, 'timetriggered': True}, {'id': 5, 'sender': 1, 'receiver': 9, 'size': 6, 'timetriggered': True}, {'id': 6, 'sender': 2, 'receiver': 6, 'size': 2, 'timetriggered': True}, {'id': 7, 'sender': 3, 'receiver': 7, 'size': 4, 'timetriggered': True}, {'id': 8, 'sender': 4, 'receiver': 8, 'size': 3, 'timetriggered': True}, {'id': 9, 'sender': 4, 'receiver': 7, 'size': 2, 'timetriggered': True}, {'id': 10, 'sender': 5, 'receiver': 9, 'size': 4, 'timetriggered': True}, {'id': 11, 'sender': 6, 'receiver': 10, 'size': 2, 'timetriggered': True}, {'id': 12, 'sender': 7, 'receiver': 10, 'size': 3, 'timetriggered': True}, {'id': 13, 'sender': 9, 'receiver': 10, 'size': 2, 'timetriggered': True}]}, {'nodes': [{'id': 0, 'is_router': False}, {'id': 1, 'is_router': False}, {'id': 2, 'is_router': False}, {'id': 3, 'is_router': True}, {'id': 4, 'is_router': True}, {'id': 5, 'is_router': True}, {'id': 6, 'is_router': True}, {'id': 7, 'is_router': True}, {'id': 8, 'is_router': True}, {'id': 9, 'is_router': True}], 'links': [{'start': 0, 'end': 7}, {'start': 1, 'end': 7}, {'start': 7, 'end': 8}, {'start': 2, 'end': 8}, {'start': 3, 'end': 8}, {'start': 4, 'end': 8}, {'start': 8, 'end': 9}, {'start': 7, 'end': 9}, {'start': 5, 'end': 9}, {'start': 6, 'end': 9}], 'frequencies': [500, 1000], 'schemes': [{'id': 0, 'wcdt': 0, 'wcct': 0, 'wccr': 1}, {'id': 1, 'wcdt': 10, 'wcct': 10, 'wccr': 0.5}]}, 0, [[9, 3, 4, 8, 10, 7, 2, 6, 0, 1, 5], [1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1], [1, 8, 5, 7, 1, 0, 5, 0, 9, 9, 8, 9, 0, 1], [6, 7, 0, 11, 2, 4, 3, 12, 1, 13, 5, 9, 8, 10]])\n",
        "\n",
        "#contextList = [(0, 0, 0, 2), (0, 1, 40, 13), (0, 2, 50, 37), (0, 3, 30, 60), (0, 4, 60, 41), (0, 5, 30, 25)]\n",
        "#Current_Context_event = 2\n",
        "#x = reconstruct_schedule_with_precedenceXX(task_allocation, node_list, processing_times, message_list, message_path_index, all_path_indexes_with_costs, message_priority_ordering,Data,contextList,Current_Context_event)\n",
        "#print(x)"
      ],
      "metadata": {
        "id": "C9Lw1C26gGym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## New GA"
      ],
      "metadata": {
        "id": "SVDYf3T6gvn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_Successor_schedule2(processing_times, message_list, all_path_indexes_with_costs,Data,contextList,Current_Context_event): # 29.09.23 adding contextList & Current_Context_event as attributes\n",
        "\n",
        "  creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))     # creating a FitnessMin class for minimization and setting the weights to (-1.0,).\n",
        "  creator.create(\"Individual\", list, fitness=creator.FitnessMin)  # creating the Individual class as a list with the FitnessMin class as the fitness attribute.\n",
        "\n",
        "  toolbox = base.Toolbox()\n",
        "\n",
        "  #parameters\n",
        "  num_tasks = len(processing_times)                               # No of jobs in task graph\n",
        "  nodes = json_data[\"platform\"][\"nodes\"]                          # Accessing the nodes of the Platform model\n",
        "  #processors = [node for node in nodes if not node[\"is_router\"]]  # Creating  a list of the processors in the Platform model.\n",
        "  num_machines = 2                               # A variable specifing the number of machines\n",
        "  num_message = len(message_list)                                 # A varible = No. of messages\n",
        "  num_paths = len(all_path_indexes_with_costs)                    # total number of paths\n",
        "\n",
        "  # Define the initialization function for the individual\n",
        "  def init_individual():\n",
        "      return random.sample(range(num_tasks), num_tasks)           # returns a list of random numbers in the range of num_tasks\n",
        "\n",
        "  #print('The initial Indidvidual is {}'.format(init_individual()))  # OMAR\n",
        "\n",
        "  # Register the initialization function in the DEAP toolbox\n",
        "  toolbox.register(\"task_order\", init_individual)\n",
        "\n",
        "  #toolbox.register(\"task_order\", tools.initIterate, creator.Individual, init_individual)\n",
        "\n",
        "  #toolbox.register(\"task_order\", random.sample, range(num_tasks), num_tasks)\n",
        "\n",
        "  # Processor_allocation value encoding\n",
        "  def processor_allocation(n_task, n_machines):\n",
        "      return [random.randint(0, num_machines) for _ in range(n_task)]   # A random list , contains the allocation of each job with the corresponding task\n",
        "  toolbox.register(\"processor_allocation\", processor_allocation, n_task=num_tasks, n_machines=num_machines)\n",
        "  HH = toolbox.processor_allocation()                                                     # OMAR\n",
        "  #print('The randomly initialized task allocation looks as the follows {}'.format(HH))    # OMAR\n",
        "\n",
        "  # Message_path_index value encoding\n",
        "  def message_path_index(n_messages, max_all_path_index):\n",
        "      return [random.randint(0, max_all_path_index - 1) for _ in range(n_messages)]  # Ensure indices are within range\n",
        "      # returns a list of random numbers of size = n_messages and values ranging from (0, max_all_path_index - 1).\n",
        "\n",
        "  toolbox.register(\"message_path_index\", message_path_index, n_messages=num_message, max_all_path_index= num_paths)\n",
        "\n",
        "  #print('The randomly initialized message path looks as the follows {}'.format(toolbox.message_path_index()))    # OMAR\n",
        "\n",
        "\n",
        "  # Message_priority_ordering permutation encoding\n",
        "  def message_priority_ordering(n_messages):\n",
        "      return random.sample(range(n_messages), n_messages)         # returns a list of size = n_messages, showing the messages ordering\n",
        "  toolbox.register(\"message_priority_ordering\", message_priority_ordering, n_messages=num_message)\n",
        "\n",
        "  #print('The random priority ordering looks as follows {}'.format(toolbox.message_priority_ordering()))        # OMAR\n",
        "\n",
        "  # Combined Individual\n",
        "  def create_individual():\n",
        "      individual = []\n",
        "      individual.extend(toolbox.task_order())\n",
        "      individual.extend(toolbox.processor_allocation())\n",
        "      individual.extend(toolbox.message_path_index())\n",
        "      individual.extend(toolbox.message_priority_ordering())\n",
        "      return individual\n",
        "  # To view the whole indivudual\n",
        "  #individual = create_individual()\n",
        "  #print(\"Individual:\", individual)\n",
        "  toolbox.register(\"individual\", tools.initIterate, creator.Individual, create_individual)\n",
        "\n",
        "  #print('The initial combined individual is {}'.format(toolbox.individual()))                  # OMAR\n",
        "\n",
        "  # Population initialization\n",
        "  toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "  #print('The initial population is {}'.format(toolbox.population(n=100)))\n",
        "\n",
        "  def reconstruct_schedule(processor_allocation, task_order):\n",
        "      # Initialize the schedule as a dictionary of empty lists\n",
        "      # where each key is a processor id\n",
        "      schedule = {i: [] for i in range(max(processor_allocation) + 1)}\n",
        "\n",
        "      # Iterate over tasks in their given order\n",
        "      for task in task_order:\n",
        "          # Assign each task to its processor\n",
        "          schedule[processor_allocation[task]].append(task)\n",
        "\n",
        "\n",
        "\n",
        "      return schedule\n",
        "  #print ('The Initial Schedule looks as {}'.format(reconstruct_schedule(toolbox.processor_allocation(), toolbox.task_order())))     # OMAR\n",
        "\n",
        "  ############################################################################################################################################################\n",
        "\n",
        "  ####################################################### Evaluation, selection, mating, and mutation methods ################################################\n",
        "\n",
        "  #########compute makespan function ################################################\n",
        "  def compute_makespan(schedule):\n",
        "      # Extract end times from the schedule\n",
        "      end_times = [info[2] for info in schedule.values()]\n",
        "      # The makespan is the maximum end time\n",
        "      makespan = max(end_times)\n",
        "      return makespan\n",
        "  ################################################################\n",
        "\n",
        "  # Define the evaluation function\n",
        "  def evaluate(individual, processing_times, message_list, all_path_indexes_with_costs):\n",
        "      # Split individual into its components\n",
        "      task_order_len = num_tasks                  # An Integer with the number of the tasks\n",
        "      processor_allocation_len = num_tasks        # Integer\n",
        "      message_path_index_len = num_message        # Integer\n",
        "      message_priority_ordering_len = num_message # Integer\n",
        "      task_order = individual[:task_order_len]    # permuation encoding, Acessing the first portion of the chromosome\n",
        "      processor_allocation = individual[task_order_len:task_order_len + processor_allocation_len] # value encoding acessing the second portion of chromosomes\n",
        "      message_path_index = individual[task_order_len + processor_allocation_len : task_order_len + processor_allocation_len + message_path_index_len] # value encoding\n",
        "      message_priority_ordering = individual[task_order_len + processor_allocation_len + message_path_index_len :] # permuation encoding\n",
        "      # print(\"task_order ================\", task_order)\n",
        "      # print(\"processor_allocation ================\", processor_allocation)\n",
        "      # print(\"message_path_index ================\", message_path_index)\n",
        "      # print(\"message_priority_ordering ================\", message_priority_ordering)\n",
        "      schedule = reconstruct_schedule_with_precedenceX(processor_allocation, task_order, processing_times, message_list, message_path_index, all_path_indexes_with_costs, message_priority_ordering)\n",
        "      makespan  = compute_makespan(schedule)\n",
        "      fitness = 1.0 / (makespan)\n",
        "      #print(\"Makespan over time is %s, %s\", makespan)\n",
        "      #scheduleR = reconstruct_schedule(processor_allocation, task_order)\n",
        "      # You would need to split the individual into its parts and use these parts to calculate the makespan\n",
        "      # For the sake of this example, let's just return the sum of all elements in the individual\n",
        "      return fitness,\n",
        "\n",
        "  # Register the evaluation function\n",
        "  toolbox.register(\"evaluate\", evaluate,processing_times=processing_times,\n",
        "                  message_list=message_list, all_path_indexes_with_costs=all_path_indexes_with_costs)\n",
        "\n",
        "  # Register selection operator\n",
        "  toolbox.register(\"select\", tools.selTournament, tournsize=3) # Applying tournament selection, tournsize=3\n",
        "                                                               # meaning three individuals will be randomly selected from the population,\n",
        "                                                               # and the best one among them will be chosen as a parent for the next generation.\n",
        "\n",
        "\n",
        "  # Register the crossover operator (Permutation encoded)\n",
        "  def mate(ind1, ind2, task_order_len):\n",
        "      # Copy the individuals\n",
        "      child1, child2 = toolbox.clone(ind1), toolbox.clone(ind2) # creating copies from ind1 and ind2 to preserve original originals while crossover and mutation.\n",
        "\n",
        "      # Apply PMX crossover to task_order\n",
        "      tools.cxPartialyMatched(child1[:task_order_len], child2[:task_order_len]) # Applying partially matched crossover method\n",
        "\n",
        "      # Return the modified individuals\n",
        "      return child1, child2\n",
        "\n",
        "  toolbox.register(\"mate\", mate, task_order_len=num_tasks)                # Register the crossover function\n",
        "\n",
        "\n",
        "  #toolbox.register(\"mate\", tools.cxTwoPoint)\n",
        "\n",
        "\n",
        "  # mutation operators\n",
        "  # def mutation_task_order(individual, task_order_len):\n",
        "  #     task_order = individual[:task_order_len] # permutation encoding\n",
        "  #     random.shuffle(task_order) # Shuffle the indexes of task_order\n",
        "  #     individual[:task_order_len] = task_order\n",
        "  #     return individual,\n",
        "  def mutation_task_order(individual, task_order_len):\n",
        "      task_order = individual[:task_order_len]  # permutation encoding\n",
        "      indices = random.sample(range(task_order_len), 2)  # Select two random indices\n",
        "      task_order[indices[0]], task_order[indices[1]] = task_order[indices[1]], task_order[indices[0]]  # Swap the elements at the selected indices\n",
        "      individual[:task_order_len] = task_order\n",
        "      return individual,\n",
        "\n",
        "  def mutation_processor_allocation(individual,task_order_len, processor_allocation_len): # (task_order_len = processor_allocation_len)\n",
        "      processor_allocation = individual[task_order_len:task_order_len + processor_allocation_len] # value encoding\n",
        "      tools.mutUniformInt(processor_allocation, low=0, up=1, indpb=0.05) # Mutate processor_allocation by uniformly changing its elements with an indpb chance\n",
        "      individual[task_order_len:task_order_len + processor_allocation_len] = processor_allocation\n",
        "      return individual,\n",
        "\n",
        "  def mutation_message_path_index(individual, task_order_len, processor_allocation_len, message_path_index_len):\n",
        "      message_path_index = individual[task_order_len + processor_allocation_len : task_order_len + processor_allocation_len + message_path_index_len] # value encoding\n",
        "      tools.mutUniformInt(message_path_index, low=0, up=1, indpb=0.05) # Mutate message_path_index by uniformly changing its elements with an indpb chance\n",
        "      individual[task_order_len + processor_allocation_len : task_order_len + processor_allocation_len + message_path_index_len] = message_path_index\n",
        "      return individual,\n",
        "\n",
        "  def mutation_message_priority_ordering(individual, task_order_len, processor_allocation_len, message_path_index_len):\n",
        "      message_priority_ordering = individual[task_order_len + processor_allocation_len + message_path_index_len :] # permuation encoding\n",
        "      tools.mutShuffleIndexes(message_priority_ordering, indpb=0.05) # Mutate message_priority_ordering by shuffling its indexes with an indpb chance\n",
        "      individual[task_order_len + processor_allocation_len + message_path_index_len :] = message_priority_ordering\n",
        "      return individual,\n",
        "\n",
        "  # register the mutation operators\n",
        "  toolbox.register(\"mutate_task_order\", mutation_task_order, task_order_len = num_tasks)\n",
        "  toolbox.register(\"mutate_processor_allocation\", mutation_processor_allocation, task_order_len = num_tasks, processor_allocation_len = num_tasks)\n",
        "  toolbox.register(\"mutate_message_path_index\", mutation_message_path_index, task_order_len = num_tasks, processor_allocation_len = num_tasks, message_path_index_len = num_message)\n",
        "  toolbox.register(\"mutate_message_priority_ordering\", mutation_message_priority_ordering, task_order_len = num_tasks, processor_allocation_len = num_tasks, message_path_index_len = num_message)\n",
        "\n",
        "  ######################################################\n",
        "  # Create an initial population\n",
        "  pop = toolbox.population(n=100)\n",
        "  # Evaluate the entire population\n",
        "  fitnesses = map(toolbox.evaluate, pop)\n",
        "  for ind, fit in zip(pop, fitnesses):\n",
        "      ind.fitness.values = fit\n",
        "  # Crossover probability and mutation probability\n",
        "  CXPB, MUTPB = 0.3, 0.2\n",
        "\n",
        "  # Extract all the fitnesses of\n",
        "  fits = [ind.fitness.values[0] for ind in pop]\n",
        "\n",
        "  NGEN = 50 #Number of generations\n",
        "  # Begin the evolution\n",
        "  for g in range(NGEN):\n",
        "      # A new generation\n",
        "      offspring = toolbox.select(pop, len(pop)) # Apply the tournmnet selection , 3 individulas will be selected\n",
        "      # Clone the selected individuals\n",
        "      offspring = list(map(toolbox.clone, offspring))\n",
        "\n",
        "      # Apply crossover and mutation on the offspring\n",
        "      for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
        "          if random.random() < CXPB:\n",
        "              toolbox.mate(child1, child2) # Applying crossover for Permutation encoding (task orders)\n",
        "              del child1.fitness.values\n",
        "              del child2.fitness.values\n",
        "      # mutation\n",
        "      for mutant in offspring:\n",
        "          if random.random() < MUTPB:\n",
        "              toolbox.mutate_task_order(mutant)\n",
        "              toolbox.mutate_processor_allocation(mutant)\n",
        "              toolbox.mutate_message_path_index(mutant)\n",
        "              toolbox.mutate_message_priority_ordering(mutant)\n",
        "              del mutant.fitness.values\n",
        "\n",
        "      # Evaluate the individuals with an invalid fitness\n",
        "      invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
        "      fitnesses = map(toolbox.evaluate, invalid_ind)\n",
        "      for ind, fit in zip(invalid_ind, fitnesses):\n",
        "          ind.fitness.values = fit\n",
        "      # The population is entirely replaced by the offspring\n",
        "      pop[:] = offspring\n",
        "  # Print the best individual\n",
        "  best_ind = tools.selBest(pop, 1)[0]\n",
        "  #print(\"Best individual is %s, %s\" % (best_ind, best_ind.fitness.values))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  task_order_len = num_tasks\n",
        "  processor_allocation_len = num_tasks\n",
        "  message_path_index_len = num_message\n",
        "  message_priority_ordering_len = num_message\n",
        "  task_order = best_ind[:task_order_len]\n",
        "  processor_allocation = best_ind[task_order_len:task_order_len + processor_allocation_len] #value encoding\n",
        "  message_path_index = best_ind[task_order_len + processor_allocation_len : task_order_len + processor_allocation_len + message_path_index_len] #value encoding\n",
        "  message_priority_ordering = best_ind[task_order_len + processor_allocation_len + message_path_index_len :] #permuation encoding\n",
        "  # print(\"task_order ================\", task_order)\n",
        "  # print(\"processor_allocation ================\", processor_allocation)\n",
        "  # print(\"message_path_index ================\", message_path_index)\n",
        "  # print(\"message_priority_ordering ================\", message_priority_ordering)\n",
        "  scheduleFinal = reconstruct_schedule_with_precedenceXX(processor_allocation, task_order, processing_times, message_list, message_path_index, all_path_indexes_with_costs, message_priority_ordering,Data,contextList,Current_Context_event)      # Editing the re-construction (New 21-09-23)\n",
        "                                                                                                                                                                                                                                                  # 29.09.23 adding contextList,Current_Context_event as attributes\n",
        "  #print('The final schedule is {}'.format(scheduleFinal))\n",
        "  Final_genome= [task_order,processor_allocation,message_path_index,message_priority_ordering]\n",
        "  return scheduleFinal , Final_genome\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7EBIoVeFgzjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ALG 1"
      ],
      "metadata": {
        "id": "DLLR03MJhHzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "############################################################\n",
        "#Construct schedule to include context path index, context message ordering\n",
        "#Begin metascheduling algorithm implementation\n",
        "\n",
        "#number of context event n\n",
        "#n = len(contextList)-1                # The first elemnt in the list is excluded as it represents the parent schedule\n",
        "\n",
        "#print(\"the value of n = \", n)\n",
        "contextList_old =[(0,0,0),(0,1,40),(0,2,50),(0,3,30),(0,4,60),(0,5,30),(0,12,40), (0,15,70), (0,18,60), (0,25,30),(0,35,60), (0,45,50)] # 80 Jobs\n",
        "\n",
        "#compute Base schedule\n",
        "SMbase, F_genome = generate_schedule_base_schedule(processing_times, message_list, all_path_indexes_with_costs)\n",
        "#index base schedule as root\n",
        "print(\"The Base Schedule Plot is :\")\n",
        "plot_schedule(SMbase)\n",
        "print('The base schedule makespan is {}'.format(compute_makespan(SMbase)))\n",
        "\n",
        "\n",
        "#Updating the Context List based on the base Schedule\n",
        "contextList = Update_Context_List(SMbase, contextList_old)            # 29.09.23\n",
        "#print(\"The Updated Context list is : \", contextList)\n",
        "# Modified on Friday 15-09-2023 Omar\n",
        "AMbase = Read_Parent_AM(json_data)\n",
        "PMbase = Read_Parent_PM(json_data)\n",
        "\n",
        "# setSMx=SMbase;// whereSMx is predecessor schedule\n",
        "SMx = SMbase\n",
        "AMx = AMbase\n",
        "PMx = PMbase\n",
        "Initial_genome = F_genome\n",
        "c = 0                                             # Context event\n",
        "volatile_context_list = contextList               # contextList =[(0,0,0),(0,1,40),(0,2,50),(0,3,30)]\n",
        "Data = [(SMbase, AMbase, PMbase, 0,Initial_genome)]\n",
        "n= 11\n",
        "def run_metascheduler(n):\n",
        "    bit_vector = [0] * n  # Initialize the bit vector with all zeros\n",
        "    #Database_Schedule_indices = []  # Initialize an empty list to store all bit vectors\n",
        "    iterate_nbit_vector_helper(bit_vector, n, 0,contextList,Data,SMx)\n",
        "\n",
        "\n",
        "# We loop through n-bit vector\n",
        "def iterate_nbit_vector_helper(bit_vector, n, index,contextList,Data,SMx):\n",
        "    if index == n:\n",
        "        if not (bit_vector == [0] * n):  # Exclude [0, 0, 0]\n",
        "            #print(bit_vector)  # Do something with the generated bit vector\n",
        "            Parent_Schedule_Bin,Parent_Schedule_dec = getparent(bit_vector)\n",
        "            #print(\"The parent schedule index is :\",Parent_Schedule_dec)\n",
        "            AMx,PMx = getPredecessors(bit_vector,Data)\n",
        "            AMy, PMy = Apply_Context(AMx,PMx,bit_vector,contextList)\n",
        "            p= bit_vector_to_value(bit_vector)\n",
        "            Current_Context_event = find_msb_position(bit_vector)      # 29.09.23 getting the index of the context event\n",
        "            print(\"The current context event is : \",Current_Context_event)\n",
        "            SMy,Genome = GA_SOLVE(AMy,PMy,SMx,Data[Parent_Schedule_dec],contextList,Current_Context_event)  # Apply GA , Passing The Dataset as an argument (21-09-23) , Through it i will access the the final genome from GA\n",
        "                                                                                            # 29.09.23 Context List added in order to appear in the Re-construction scope\n",
        "                                                                                            # 29.09.23 Current_Context_event added to know what event should be applied in the reconstruction\n",
        "            schedule_makespan = compute_makespan(SMy)\n",
        "            #print(\"The makespan of this schedule is: \",schedule_makespan)\n",
        "            #plot_schedule(SMy)\n",
        "\n",
        "            #p= bit_vector_to_value(bit_vector)\n",
        "\n",
        "            D=(SMy,AMy,PMy,p,Genome,schedule_makespan)\n",
        "            Data.append(D)\n",
        "            #print(\"The Data_base is : \" , Data)\n",
        "\n",
        "\n",
        "        return\n",
        "\n",
        "    # Recursively generate all possible combinations\n",
        "    bit_vector[index] = 0\n",
        "    iterate_nbit_vector_helper(bit_vector, n, index + 1,contextList,Data,SMx)\n",
        "\n",
        "    bit_vector[index] = 1\n",
        "    iterate_nbit_vector_helper(bit_vector, n, index + 1,contextList,Data,SMx)\n",
        "\n",
        "run_metascheduler(n)"
      ],
      "metadata": {
        "id": "mNuo2IJWhLBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(Data)"
      ],
      "metadata": {
        "id": "0ZD5B7m8AZAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculating the communication cost"
      ],
      "metadata": {
        "id": "Yn9-QdYpt1pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print(Data)\n",
        "print(Data[1])\n",
        "print(len(Data))"
      ],
      "metadata": {
        "id": "8qW01QM6t7kN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "makespan_list = []\n",
        "for i in range(1,2 ** (n)):\n",
        "  makespan_list.append(Data[i][-1])\n",
        "\n",
        "print(makespan_list)\n",
        "\n",
        "\n",
        "sampling_freq = 10\n",
        "context_information_cost = 5\n",
        "\n",
        "result_list = []\n",
        "\n",
        "for element in makespan_list:\n",
        "    result = round((element / sampling_freq) * context_information_cost)\n",
        "    result_list.append(result)\n",
        "\n",
        "\n",
        "\n",
        "print(result_list)\n",
        "\n",
        "print(sum(result_list))"
      ],
      "metadata": {
        "id": "lZyjeHKdt-tf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(makespan_list)"
      ],
      "metadata": {
        "id": "RONocQqxv_uL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EN9vtGr1H7BS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}